%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num, iicol]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
\usepackage[numbers]{natbib}
\usepackage{threeparttable}
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>
\RequirePackage{amsmath}
%
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{wrapfig}

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{tabularx} % Para criar tabelas com largura definida
\usepackage{array}    % Para alinhamento customizado (necessário para o \raggedright)
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%


\usepackage{glossaries}
\usepackage{glossaries-extra}
\setabbreviationstyle[acronym]{long-short}

\newacronym{AI}{AI}{Artificial Intelligence}
\newacronym{CV}{CV}{Cross-Validation}
\newacronym{DL}{DL}{Deep Learning}
\newacronym{EER}{EER}{Equal Error Rate}
\newacronym{ZSL}{ZSL}{Zero-Shot Learning}
\newacronym{GZSL}{GZSL}{Generalized Zero-Shot Learning}
\newacronym{LLM}{LLM}{Large Language Model}
\newacronym{LVLM}{LVLM}{Large Vision-Language Model}
\newacronym{OCR}{OCR}{Optical Character Recognition}
\newacronym{VDM}{VDM}{Visual Document Matching}
\newacronym{LO-CDIP}{LA-CDIP}{Layout-Aware Complex Document Information Processing}
\newacronym{LA-ZSL}{LA-ZSL}{Layout-Aware Zero-Shot Learning}
\newacronym{VLR}{VLR}{Visual Layout Recognition}
\newacronym{SGD}{SGD}{Stochastic Gradient Descent}
\newacronym{CDP}{CDP}{Copy Detection Patterns}
\newacronym{DLA}{DLA}{Document Layout Analysis}
\newacronym{ViT}{ViT}{Vision Transformer}
\newacronym{DLR}{DLR}{Document Layout Recogition}
\newacronym{FAR}{FAR}{False Acceptance Rate}
\newacronym{FRR}{FRR}{False Rejection Rate}
\newacronym{DIC}{DIC}{Document Image Classification}
\newacronym{ZS-DIC}{ZS-DIC}{Zero-Shot Document Classification}
\newacronym{FS-DIC}{FS-DIC}{Few-Shot Document Classification}

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{CaVL-Doc: Comparative Aligned Vision-Language Document Embeddings for Zero-Shot DIC}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

% \author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

% \author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
% \equalcont{These authors contributed equally to this work.}

% \author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
% \equalcont{These authors contributed equally to this work.}

% \affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

% \affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

% \affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
Large Vision-Language Models (LVLMs) have demonstrated impressive zero-shot capabilities in document understanding. However, their direct application to specialized, high-stakes enterprise classification tasks is often limited by performance saturation and the high computational cost of full fine-tuning. In this work, we introduce CaVL-Doc (Comparative Aligned Vision-Language Document Embeddings), a lightweight adaptation framework that leverages the robust multimodal alignment of pre-trained LVLMs without retraining the backbone. We propose a specialized architecture that processes aligned multimodal tokens using Multi-Query Attention Pooling and a Residual Projection Head. Furthermore, we address the challenge of intra-class variance in document images through a robust two-phase curriculum learning strategy. We systematically evaluate "Elastic" variants of angular margin losses (ElasticArcFace, ElasticCosFace, ElasticCircle) within this framework, introducing stochastic margins and hard negative mining to prevent overfitting and encourage a more generalizable metric space. Extensive experiments on the LA-CDIP and RVL-CDIP datasets demonstrate that our approach, applied to a 2B parameter model, achieves state-of-the-art performance, significantly outperforming larger proprietary models while maintaining high inference efficiency.
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Few-Shot Document Image Classification, Metric Learning, Curriculum Learning, LVLM, Embedding Space Learning}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

% --- Início da sua Introdução ---

\section{Introduction}\label{sec:introduction}

The field of Document Understanding encompasses the analysis of content and structure across diverse formats, including text, images, tables, and graphics \cite{formUnderstandingSurvey}. Within this spectrum, accurate \glsfirst{DIC} is fundamental for organizations to ensure compliance and automate high-volume workflows. However, the complexity of \gls{DIC} is exacerbated by the dynamic nature of real-world documents: layouts evolve, new templates emerge, and traditional classification models frequently become obsolete, necessitating costly retraining cycles to maintain relevance \cite{macedo2025vdm}.

Historically, \gls{DIC} evolved from structure-based heuristics to deep visual methods and, more recently, to hybrid multimodal approaches. Currently, the field is addressing the critical challenge of learning from scarce data, with \glsfirst{ZS-DIC} (Zero-Shot) and \glsfirst{FS-DIC} (Few-Shot) emerging as essential paradigms to handle the rapid turnover of document types \cite{macedo2025vdm,voerman2025optimizing,bakkali2025globaldoc}.

The advent of multimodal \glsfirst{LVLM} has provided a powerful mechanism for these low-data scenarios. By leveraging vast pre-training knowledge, LVLMs can achieve competitive performance with simple textual prompts \cite{scius2024zeroshot}, shifting the paradigm from ``training from scratch'' to ``efficient adaptation.'' However, despite this potential, two barriers limit the deployment of LVLMs in enterprise-grade pipelines:

\begin{enumerate}
    \item \textbf{Performance Saturation:} While prompt engineering offers a strong baseline, it often hits a performance ceiling. Achieving the reliability required for critical business processes demands robust adaptation to subtle, domain-specific visual nuances that standard prompts fail to capture \cite{scius2024zeroshot}.
    \item \textbf{Operational Sovereignty and Efficiency:} The reliance on large proprietary models raises concerns regarding financial sustainability \cite{patel2024characterizing, cruz2025innovating}, energy footprint \cite{patterson2022carbon}, and data privacy. Regulatory landscapes increasingly demand solutions that ensure technological autonomy and can be deployed on-premise \cite{wiest2024privacy, strong2025trustworthy}, creating an urgent need for efficient, open-source foundation models.
\end{enumerate}

To address these barriers, the state-of-the-art in efficient \gls{FS-DIC} is converging towards Metric Learning—such as Prototypical Networks \cite{voerman2025optimizing} or Siamese architectures \cite{macedo2025vdm}. These methods aim to learn a specialized embedding space where similar documents are grouped together. However, standard metric learning approaches often fail to enforce strict geometric constraints. They treat training samples equally, frequently resulting in embedding spaces with loose intra-class compactness and ambiguous decision boundaries, especially when facing the high visual variance typical of administrative documents.

% --- FIGURA CONCEITUAL (SIMÉTRICA) ---
\begin{figure}[t]
\centering
% Certifique-se de que o arquivo metric_space_symmetric.pdf está na pasta correta
\includegraphics[width=\columnwidth]{../assets/metric_space.png}
\caption{Conceptual visualization of the Zero-Shot learning objective. The figure illustrates the goal of constructing a metric space using seen classes that maintains discriminative power for unseen classes, allowing new document types to form distinct, separable classes without additional training.}
\label{fig:metric-space-concept}
\end{figure}

This limitation reveals a critical gap: the need for a framework that not only adapts an LVLM but actively structures its latent space. As illustrated in Figure \ref{fig:metric-space-concept}, a robust classification model must do more than cluster data; it must enforce \textbf{high intra-class compactness} (minimizing variance within a document type) and \textbf{large inter-class angular margins} (ensuring separation between types). This geometric structuring during the training phase (Phase 1) is what safeguards the model's ability to project unseen classes into distinct, non-overlapping regions during inference (Phase 2).

In this work, we introduce \textbf{CaVL-Doc} (Comparative Aligned Vision-Language Document Embeddings), a novel framework for robust few-shot adaptation. Unlike standard approaches that treat the backbone as a black box, we design a specialized architecture that aggregates aligned multimodal tokens using Multi-Query Attention Pooling. Crucially, to solve the geometric gap, we move beyond standard contrastive losses. We implement and systematically evaluate a family of ``Elastic'' angular margin losses (e.g., ElasticArcFace, ElasticCircle). These losses introduce a stochastic margin during training, preventing the model from overfitting to the limited support set and forcing the learning of a generalized metric space that respects the geometry shown in Figure \ref{fig:metric-space-concept}.

We demonstrate that CaVL-Doc, applied to a 2B parameter open-source LVLM (InternVL2), achieves state-of-the-art few-shot performance, significantly outperforming standard metric learning baselines while maintaining a compact operational footprint suitable for sovereign deployment.

\subsection{The main contributions}
The key contributions of this work are summarized as follows:
\begin{enumerate}
    \item \textbf{First Fully Multimodal Metric for DIC:} We introduce the first metric learning framework designed to operate directly on the aligned vision-language latent space of LVLMs. By bypassing traditional unimodal feature extraction or late fusion, CaVL-Doc constructs a semantically rich embedding space that inherently captures the interplay between visual layout and textual content.
    
    \item \textbf{Advancing Zero-Shot DIC Research:} We contribute to the underexplored field of Zero-Shot Document Image Classification (ZS-DIC) by proposing a robust methodology for adapting general-purpose models to specialized domains. Our approach enables the effective classification of unseen document types, addressing the critical "cold start" problem in enterprise document processing.
    
    \item \textbf{Extensive Architectural Evaluation:} We conduct a comprehensive experimental analysis to determine the optimal architectural components for multimodal document embeddings. This includes a rigorous evaluation of pooling mechanisms (Multi-Query Attention), loss functions (Elastic vs. Static), and sampling strategies, providing a blueprint for future research in multimodal metric learning.
    
    \item \textbf{Robust Training Strategy:} We propose a hybrid optimization strategy that combines a two-phase Curriculum Learning schedule with an RL-based "Professor" for active hard negative mining, ensuring stable convergence and maximizing data efficiency in few-shot scenarios.
    
    \item \textbf{SOTA Performance with Efficiency:} We empirically demonstrate that our adapted 2B parameter model achieves state-of-the-art performance on the LA-CDIP and RVL-CDIP benchmarks. We show that specialized metric adaptation allows compact open-source models to outperform massive proprietary models, enabling sovereign and efficient deployment.
\end{enumerate}

The remainder of this article is structured as follows: Section~\ref{sec:related-work} reviews the state-of-the-art in few-shot document classification. Section~\ref{sec:methodology} details the CaVL-Doc framework, the token alignment strategy, and the Elastic Margin Losses. Section~\ref{sec:experiments} describes the experimental setup and baselines. Section~\ref{sec:results} presents the empirical analysis. Finally, Section \ref{sec:conclusion} concludes the article and outlines future directions.
% --- Fim da Introdução ---

\section{Related Work}\label{sec:related-work}
This work is positioned at the intersection of several key research areas within document analysis and machine learning. To provide a comprehensive background for our proposed CaVL-Doc framework, this section reviews the foundational and recent advancements in four critical domains:
(1) The evolution of Document Image Classification (DIC) toward Few-Shot (FSL) paradigms; 
(2) The application of Large Vision-Language Models (LVLMs) as fixed-backbone feature extractors; 
(3) The adoption of Metric Learning as the state-of-the-art for efficient FSL in documents; and 
(4) The evolution of loss functions for robust embedding learning.

\subsection{Document Classification: From ZS-DIC to Few-Shot Adaptation}\label{sec:rw_dic}
This section reviews the evolution of Document Image Classification (DIC), highlighting the transition from traditional supervised methods \cite{liu_document_2021} to the challenges of data-scarce environments. The advent of LVLMs initially established powerful baselines for Zero-Shot Document Classification (ZS-DIC) through simple prompting \cite{scius2024zeroshot}.

However, as noted in recent literature, ZS-DIC often saturates in performance and lacks the precision required for specialized enterprise tasks \cite{scius2024zeroshot}. This has shifted the research focus to the more practical challenge of Few-Shot Learning (FSL) \cite{bakkali2025globaldoc, voerman2025optimizing}. The goal of FSL is to efficiently adapt a model to new, unseen document classes using only a handful of examples. This scenario, which balances performance with the high cost of data annotation, is the primary focus of our work.

\subsection{Metric Learning for FSL Document Classification}\label{sec:rw_dml}
While full fine-tuning of LVLMs is one FSL approach \cite{scius2024zeroshot}, it remains computationally expensive. A more efficient and dominant strategy in the recent FSL document literature is Deep Metric Learning (DML) \cite{bakkali2025globaldoc, voerman2025optimizing, macedo2026vdm}.

DML aims to learn a discriminative embedding space—typically using a lightweight "projection head" over fixed LVLM features \cite{shu2024dsncl}—where similar samples are pulled closer and dissimilar samples are pushed apart \cite{shu2024dsncl, yan2024lmmetric}. The state-of-the-art in FSL for documents has converged on this approach. For instance, Voerman et al. conduct a comparative analysis for identity document classification and conclude that Prototypical Networks (a classic DML method) are the most practical and effective FSL solution \cite{voerman2025optimizing}. Similarly, Bakkali et al. define their FSL task using a Prototypical Network, which calculates a class centroid from the support set embeddings \cite{bakkali2025globaldoc}. Other works, such as Macedo et al., achieve the same goal using Siamese Networks with a standard Contrastive Loss \cite{macedo2025vdm}.

However, this reliance on standard DML methods exposes a critical gap: these techniques treat all training pairs equally. They struggle with high intra-class variance and complex negative pairs, leading to sub-optimal generalization and what Voerman et al. describe as a "precision issue" \cite{voerman2025optimizing}. Our work addresses this specific gap.

\subsection{Robust Loss Functions for Metric Learning}\label{sec:rw_cl}
The second axis of our framework focuses on optimizing the similarity metric itself. This is a common objective in Deep Metric Learning (DML), which aims to learn a discriminative embedding space where similar samples are pulled closer together and dissimilar samples are pushed far apart \cite{shu2024dsncl, yan2024lmmetric}. Classic DML objectives for retrieval tasks often rely on Contrastive Loss \cite{shu2024dsncl} or Triplet Loss \cite{yan2024lmmetric}.

However, these Euclidean-based losses often fail to enforce sufficient intra-class compactness. To address this, angular margin losses were introduced, such as ArcFace, CosFace, and Circle Loss. These losses project features onto a hypersphere and enforce an angular margin penalty, leading to more discriminative features. Recently, "Elastic" variants of these losses have been proposed to handle data with high noise or variance. By treating the margin as a random variable sampled from a distribution, rather than a fixed hyperparameter, these losses prevent the model from overfitting to easy samples or noisy labels, promoting a more robust decision boundary. Our work systematically applies and evaluates these elastic losses in the context of few-shot document classification.

% --- SEÇÃO DE METODOLOGIA REESCRITA ---

\section{The CaVL-Doc Framework}\label{sec:methodology}

Our framework is designed for efficient Few-Shot Document Classification (FSL). It consists of a frozen LVLM backbone, a Multi-Query Attention Pooling mechanism, and a Residual Projection Head trained with Elastic Margin Losses.

% TEXTO ENXUTO: Apenas descreve a entrada e saída.
The foundation of our framework is the InternVL3-2B model \cite{internvl2024}. Given an input document image $x$, we extract the sequence of $N$ aligned multimodal tokens, $T = \{t_1, t_2, \dots, t_N\}$, from the last hidden layer. These tokens represent rich, localized semantic features aligned with the model's language understanding. To prevent catastrophic forgetting, the backbone remains \textbf{frozen}.
\subsection{Architecture: Multi-Query Attention and Residual Head}\label{sec:architecture}

Standard metric learning approaches often use a simple Mean Pooling followed by a Linear Layer. We argue that this destroys the fine-grained semantic information present in the token sequence $T$. To address this, CaVL-Doc employs a specialized two-stage architecture.

\subsubsection{Multi-Query Attention Pooling}
To capture the diverse semantic aspects of a document, we introduce a Multi-Query Attention Pooling mechanism. Instead of condensing the document into a single vector, we define a set of $Q$ learnable query vectors, $Q = \{q_1, \dots, q_Q\}$.

For each query $q_j$, the mechanism computes an attention score over the input tokens $T$:
\begin{equation}
    \alpha_{j,i} = \text{softmax}\left(\frac{q_j \cdot t_i^T}{\sqrt{d}}\right)
\end{equation}
The resulting pooled vector $h_j$ is a weighted sum of the tokens: $h_j = \sum_{i=1}^{N} \alpha_{j,i} t_i$. This allows the model to learn distinct "views" of the document (e.g., one query might focus on header information, another on visual layout). These $Q$ vectors are then concatenated to form a comprehensive document representation.

\subsubsection{Residual Projection Head}
The aggregated features are then processed by a \textbf{Residual Projection Head} ($\mathcal{G}_\phi$). Standard Multi-Layer Perceptrons (MLPs) can sometimes distort the well-structured geometry of the pre-trained feature space. To mitigate this, we employ a residual connection:
\begin{equation}
    v' = v + \text{MLP}(v)
\end{equation}
where $v$ is the concatenated output of the attention pooling. This residual structure ensures that the original, robust LVLM features are preserved as a baseline, while the MLP learns only the necessary non-linear transformations to adapt the metric space to the specific document classification task.

A critical advantage of this architecture is its alignment with the concept of "Once Learning" \cite{weigang1999study}. Drawing inspiration from biological neural processing, where recognition often occurs as a rapid, parallel integration of stimuli rather than a sequential reconstruction, this paradigm contrasts sharply with standard LVLM usage. In the standard approach, comparing documents involves prompting the model and waiting for it to generate a textual response via autoregressive token decoding. This process is inherently sequential, akin to a slow, deliberative reasoning chain.

In contrast, CaVL-Doc captures the entire semantic structure of the document—represented by the sequence of aligned multimodal tokens—in a single, holistic forward pass. This "at once" aggregation transforms the complex token sequence into a unified metric representation without the latency of sequential generation. The comparison is then reduced to a direct metric operation in the embedding space, which is orders of magnitude faster and scalable to large databases.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{../assets/architecture_overview.png}
\caption{Overview of the CaVL-Doc framework. The architecture leverages aligned multimodal tokens from the frozen InternVL3 backbone. These tokens are aggregated via Multi-Query Attention Pooling and processed by a Residual Projection Head, which is trained using Elastic Margin Losses to ensure a robust embedding space.}
\label{fig:cavl-structure}
\end{figure}

\subsection{The "Professor": Active Data Selection Agent}\label{sec:professor}
% (Mover a descrição do AGENTE RL para cá, pois é um COMPONENTE do método)
To enhance learning efficiency, CaVL-Doc incorporates a lightweight auxiliary network, the "Professor". This RL-based agent observes the Student model's loss distribution and actively selects the most informative pairs (hard negatives) for training, replacing standard random sampling.

\subsection{Objective Functions for Metric Alignment}\label{sec:losses}

To adapt the projection head $\mathcal{G}_\phi$ effectively, the training objective must enforce strict intra-class compactness and large inter-class separability. Since the optimal geometric constraints for few-shot document classification remain an open research question, our framework is designed to support and evaluate a comprehensive spectrum of metric learning objectives, ranging from Euclidean distance optimization to advanced angular margin constraints.

\subsubsection{Euclidean Distance Constraints}
Traditional Zero-Shot approaches often rely on optimizing direct distance metrics in Euclidean space. We incorporate two foundational formulations that serve as strong baselines for establishing global manifold structure:

\begin{itemize}
    \item \textbf{Contrastive Loss:} This formulation operates on pairs, pulling positive samples $(x_i, x_j)$ closer while pushing negative pairs beyond a fixed distance margin $\alpha$. It is effective for rapid convergence but treats all negative samples equally once the margin is satisfied.
    \item \textbf{Triplet Loss:} Operating on triplets (anchor $a$, positive $p$, negative $n$), this loss enforces the relative constraint $d(a, p) < d(a, n) - \alpha$. By focusing on the relative geometry rather than absolute distances, it often captures local structure better than Contrastive Loss.
\end{itemize}

\subsubsection{Angular Margin Constraints}
To address the limitations of Euclidean constraints—which may lack explicit bounds on the hypersphere surface—we investigate the Angular Margin family (e.g., ArcFace, CosFace). These methods normalize features and weights, projecting them onto a hypersphere and enforcing penalties in the angular domain.

Formally, given a target angle $\theta_{y_i}$ between the feature vector and the class center, an angular margin loss introduces a penalty $m$:
\begin{equation}
    \mathcal{L}_{Ang} = -\log \frac{e^{s \cdot \cos(\theta_{y_i} + m)}}{e^{s \cdot \cos(\theta_{y_i} + m)} + \sum_{j \neq y_i} e^{s \cdot \cos(\theta_{j})}}
\end{equation}
This enforces a stricter decision boundary, theoretically pushing samples of the same class closer together in terms of cosine similarity.

\subsubsection{Stochastic Margins for Robustness (Elasticity)}
Finally, we explore the hypothesis that fixed margins may lead to overfitting in low-data regimes. To investigate this, we formulate a stochastic variant of the angular loss (often termed "Elastic"), where the margin $m$ is treated as a random variable sampled from a distribution $m \sim \mathcal{N}(\mu, \sigma^2)$ at each step. This mechanism aims to prevent the model from collapsing onto rigid boundaries defined by scarce support samples. 

Our experimental protocol (Section \ref{sec:experiments}) systematically compares these formulations—Euclidean, Fixed Angular, and Stochastic Angular—to empirically determine the most robust objective for unseen document classes.
% --- SEÇÃO DE EXPERIMENTOS REESCRITA ---

\section{Experimental Setup}\label{sec:experiments}

To validate our proposed \textbf{CaVL-Doc} framework, we conduct a series of experiments designed to measure the impact of our architectural choices and the effectiveness of the Elastic Margin losses. This section details the datasets, evaluation protocols, and implementation settings.

%------------------------------------------------
\subsection{Datasets and Evaluation Metrics}
%------------------------------------------------

We evaluate our framework on two standard, large-scale document classification benchmarks.

\subsubsection{Datasets: LA-CDIP and RVL-CDIP}
We use two public document image datasets for our experiments:
\begin{itemize}
    \item \textbf{LA-CDIP} \cite{macedo2025vdm}: This is our primary evaluation dataset. It is a reorganization of the RVL-CDIP database, comprising \textbf{4,993 documents across 144 classes}, specifically curated to emphasize visual structure over semantic information.
    
    \item \textbf{RVL-CDIP} \cite{harley2015rvlcdip}: A widely-used benchmark consisting of 400,000 document images across 16 classes. This dataset has been recently benchmarked and extended with standardized Zero-Shot Learning (ZSL) \cite{sinha2024cica} and Few-Shot Learning (FSL) \cite{scius2024zeroshot} protocols.
\end{itemize}

For both datasets, we follow a standard Few-Shot Learning protocol. We use the official training splits to train our metric learning head. We report all final performance on the official validation/test sets.

% --- Subseção de Métricas de Avaliação ---
\subsection{Evaluation Metrics}

We evaluate our framework using two distinct metrics to capture both the discriminative power of the embedding space and its practical classification utility.

\subsubsection{Pair-wise Verification (EER)}
Given our pair-wise matching setup, we evaluate performance using the Equal Error Rate (EER). The EER is the point on the Receiver Operating Characteristic (ROC) curve where the False Acceptance Rate (FAR) equals the False Rejection Rate (FRR). 

A lower EER indicates a more discriminative model, as it represents the lowest achievable error rate when the acceptance threshold is set to equalize false positives and false negatives. This metric is standard for zero-shot verification tasks as it provides a single, threshold-independent measure of the separability between positive pairs (same class) and negative pairs (different classes).

\subsubsection{One-Shot Classification Accuracy (Top-1 Acc.)}
To measure the practical utility of the learned metric space for classification, we also report the Top-1 One-Shot Classification Accuracy. This protocol follows a standard Zero-Shot Learning (ZSL) setup applied to unseen classes.

For each of the $K$ \textit{unseen} classes in the test set, we randomly select a single image to serve as the class prototype (the "support" sample). The remaining images are used as the "query" set. A query image $x_q$ is classified by finding the class $k$ whose prototype $v_k$ is closest in the learned metric space:
\begin{equation}
    \hat{c} = \underset{k \in \{1, \dots, K\}}{\arg\min} \mathcal{S}(\mathcal{G}(v_q; \phi), \mathcal{G}(v_k; \phi))
\end{equation}
where $\mathcal{S}$ is the distance/similarity metric (e.g., Cosine or Euclidean). Accuracy is calculated as the percentage of query images correctly assigned to their true class label ($\hat{c} = c_{true}$) among the $K$ unseen candidates. This metric directly assesses the model's ability to generalize to novel document types without requiring retraining.

% \subsection{Training Strategy (Curriculum Learning)}

% To further enhance the generalization capability of our model, particularly for unseen classes, we propose a robust training strategy that combines a Macro-Level Curriculum (scheduling the loss function) with a Micro-Level RL Agent (scheduling the data distribution).

% \subsubsection{Macro-Level: Two-Phase Loss Schedule}
% Instead of training with a static objective throughout, we divide the optimization process into two distinct phases. This strategy is designed to first establish a stable global structure and then refine the local decision boundaries.

% \begin{enumerate}
%     \item \textbf{Phase 1: Global Structure Organization (Triplet Loss).} The primary objective of the first phase is to rapidly organize the embedding space. We employ Triplet Loss, which is computationally efficient and effective at pulling positive pairs together while pushing negatives apart, establishing the global topology of the data manifold.
    
%     \item \textbf{Phase 2: Local Boundary Refinement (Elastic Angular Loss).} Once the global structure is established, we switch to an Elastic Angular Margin Loss (e.g., Elastic CosFace). This phase enforces strict, yet robust, angular separability between classes. The stochastic margin ($m \sim \mathcal{N}(\mu, \sigma^2)$) prevents overfitting to the support set and encourages a more generalizable metric space.
% \end{enumerate}

% \subsubsection{Micro-Level: RL-Based Data Selection (The Professor)}
% Crucially, unlike traditional curriculum learning that relies on fixed heuristics (e.g., "start with easy samples"), our framework employs a Reinforcement Learning agent (the "Professor") to actively select training data \textit{throughout all three phases}.

% The Professor is a lightweight policy network that observes the current state of the Student model (represented by the loss distribution of a candidate batch) and selects the most informative pairs for training. This ensures that whether the Student is in the "Alignment" phase or the "Refinement" phase, it is always training on the optimal set of samples (e.g., hard negatives) required to maximize its learning progress at that specific moment. This hybrid approach combines the stability of a loss curriculum with the adaptability of RL-based hard mining.

%------------------------------------------------
\subsection{Implementation Details}
%------------------------------------------------

All experiments are conducted using PyTorch on a system equipped with NVIDIA GPUs.

\begin{itemize}
    \item \textbf{LVLM Backbone}: We use the \texttt{InternVL3-2B} model as our frozen feature extractor $\mathcal{F}_\theta$. We extract the sequence of aligned multimodal tokens from the last hidden layer. The backbone model is loaded in 16-bit precision.
    
    \item \textbf{Metric Head Training:} The \texttt{ProjectionHead} $\mathcal{G}_\phi$ is trained using the AdamW optimizer with a learning rate of $1 \times 10^{-4}$ and a weight decay of $1 \times 10^{-4}$. The projection output dimension is set to 1536. We use a batch size of 64 and train for 5 epochs, reflecting the efficiency of our few-shot adaptation approach.
    
    \item \textbf{Loss Functions:} We evaluate a diverse set of objective functions, ranging from Euclidean-based losses (Contrastive, Triplet) to Angular Margin losses (ArcFace, CosFace). For angular losses, we use a fixed scaling factor $s=30$. Specific hyperparameters such as margins ($m$), elasticity ($\sigma$), and sub-centers ($k$) are varied according to the experimental protocol to determine the optimal configuration.
\end{itemize}

%------------------------------------------------
\subsection{Baselines and Comparison Methods}
%------------------------------------------------

To validate the effectiveness of CaVL-Doc, we benchmark it against a hierarchy of methods, ranging from foundational baselines to established state-of-the-art results reported in the literature:

\begin{itemize}
    \item \textbf{Pixel-Baseline (Naïve):} A non-parametric baseline that performs pair-wise classification based on the Euclidean distance of raw flattened pixel vectors. This serves as a sanity check to quantify the complexity of the visual tasks \cite{macedo2026vdm}.
    
    \item \textbf{Base-LVLM (Zero-Shot):} This baseline evaluates the raw representational power of the \textit{frozen} $\mathcal{F}_\theta$ (InternVL3-2B) without any adaptation. We extract global features using standard Mean Pooling and perform classification using Cosine Similarity. This represents the performance floor of our specific backbone without the proposed framework.
    
    \item \textbf{State-of-the-Art (SOTA) Baselines:} We compare our results against recent benchmarks reported in the literature for Zero-Shot and Few-Shot document classification. 
    \begin{itemize}
        \item For \textbf{RVL-CDIP}, we include results from recent multimodal ZSL approaches (e.g., \cite{scius2024zeroshot, sinha2024cica}), which utilize large-scale pre-training adapted for document understanding.
        \item For \textbf{LA-CDIP}, we report the strong baselines established in the dataset's introduction paper \cite{macedo2025vdm}, focusing on standard metric learning approaches.
    \end{itemize}
        
    \item \textbf{Ours (CaVL-Doc):} The full proposed framework, featuring the Multi-Query Attention Pooling architecture and the Residual Head. We report the performance using the optimal objective function configuration identified through the systematic ablation study described in Section \ref{sec:protocol}.
\end{itemize}

\subsection{Experimental Protocol}\label{sec:protocol}

To systematically validate the components of our framework and identify the optimal configuration for few-shot document classification, we designed a six-phase experimental protocol. This structured approach allows us to isolate the contribution of each module—from the loss function to the sampling strategy—ensuring that the final performance is a result of informed design choices rather than random hyperparameter tuning.

\subsubsection{Objective Function Benchmarking}
\textbf{Objective:} Identify the loss formulation that yields the most stable optimization landscape and highest zero-shot generalization for the InternVL backbone.

\textbf{Setup:} To isolate the impact of the loss function, we fix the architecture with $Q=4$ queries and enable the "Professor" mechanism to ensure all losses benefit from hard negatives. The model is trained on LA-CDIP and validated on the RVL-CDIP Zero-Shot split.

\textbf{Experiments:} We compare four distinct metric learning families:
\begin{itemize}
    \item \textbf{Contrastive Loss:} Standard Euclidean distance baseline.
    \item \textbf{Triplet Loss:} Relative distance baseline, known for efficient global clustering.
    \item \textbf{ArcFace \& CosFace:} Angular margin baselines, testing geometric constraints on the hypersphere.
\end{itemize}
\textbf{Decision Criterion:} We select the "engine" that minimizes validation volatility (loss smoothness) while maximizing the Peak Zero-Shot Accuracy.

\subsubsection{Sampling Strategy Ablation}
\textbf{Objective:} Quantify the impact of the proposed "Professor" agent (Active Hard Mining) compared to standard data sampling.

\textbf{Setup:} Using the optimal loss function from the previous section, we contrast two training regimes:
\begin{itemize}
    \item \textbf{Random Sampling:} Standard stochastic gradient descent where negative pairs are sampled uniformly from the dataset.
    \item \textbf{Professor Agent:} An active learning approach where the agent selects the top-k hardest negatives (e.g., 8 out of 64 candidates) based on the Student's current state.
\end{itemize}
\textbf{Analysis:} We evaluate the impact on convergence speed (training efficiency) and the discriminative power of the final embedding space.

\subsubsection{Architectural Capacity Analysis}
\textbf{Objective:} Determine the optimal information bottleneck size for the Multi-Query Attention mechanism.

\textbf{Setup:} We vary the number of learnable queries ($Q$) in the attention pooler. This parameter controls the granularity of the document representation.

\textbf{Experiments:} We test $Q \in \{4, 8, 16\}$.
\begin{itemize}
    \item \textbf{Hypothesis:} A small $Q$ forces semantic compression (reducing noise), while a large $Q$ captures fine-grained details. We aim to find the $Q_{BEST}$ that maximizes performance without incurring unnecessary computational overhead.
\end{itemize}

\subsubsection{Geometric Constraint Optimization}
\textbf{Objective:} Refine the topology of the embedding space by tuning the decision boundaries for intra-class variance.

\textbf{Setup:} Using the optimal architecture ($Q_{BEST}$) and sampler (Professor), we explore advanced geometric hyperparameters in two steps:

\textbf{Experiments:}
\begin{itemize}
    \item \textbf{Margin Magnitude ($m$):} We first determine the optimal static margin strength by testing conservative ($0.35$), balanced ($0.45$), and aggressive ($0.55$) values to find $M_{BEST}$.
    \item \textbf{Dynamic Constraints:} Fixing $M_{BEST}$, we then test structural variations:
    \begin{itemize}
        \item \textbf{Sub-center (k=3):} Allowing multiple centroids per class to handle distinct layout templates within the same category.
        \item \textbf{Elasticity (Stochastic Margin):} Replacing the fixed margin with a distribution $m \sim \mathcal{N}(M_{BEST}, \sigma^2)$ to assess robustness against overfitting in few-shot scenarios.
    \end{itemize}
\end{itemize}

\subsubsection{Sample Size Analysis}
\textbf{Objective:} Evaluate the data efficiency of the framework and determine the minimum number of samples required for robust adaptation.

\textbf{Setup:} Using the optimal configuration ($Q_{BEST}$, Professor, Elastic Margin), we train the model on subsets of the LA-CDIP dataset with varying sizes.

\textbf{Experiments:} We test training set sizes of $N \in \{2500, 5000, 10000\}$ documents.
\begin{itemize}
    \item \textbf{Hypothesis:} While performance is expected to improve with more data, we aim to identify the point of diminishing returns to validate the few-shot efficiency of our approach.
\end{itemize}

\subsubsection{Curriculum Optimization Strategy}
\textbf{Objective:} Validate the proposed two-stage hybrid training curriculum against single-stage training.

\textbf{Hypothesis:} Global structure (topology) should be established before enforcing strict local boundaries (geometry).

\textbf{Strategy:}
\begin{itemize}
    \item \textbf{Stage 1 (Manifold Organization):} We train initially using Triplet Loss to rapidly pull similar documents together and push dissimilar ones apart based on relative distance.
    \item \textbf{Stage 2 (Boundary Refinement):} We load the weights and transition to the Best Angular Configuration (identified in the previous section) with a reduced learning rate. This phase sharpens the decision boundaries for precise classification.
\end{itemize}

\section{Results and Discussion}\label{sec:results}

In this section, we present the empirical evaluation of the CaVL-Doc framework. We follow a bottom-up approach: first, we validate our architectural design and objective functions through a systematic ablation study. Then, using the optimal configurations identified, we benchmark CaVL-Doc against state-of-the-art models on the LA-CDIP and RVL-CDIP datasets.

% =========================================================================================
% SUBSECTION: ABLATION STUDY (ESTUDOS E EXPERIMENTOS)
% =========================================================================================
\subsection{Step-by-Step Ablation Analysis}\label{sec:ablation}

To determine the optimal configuration for the CaVL-Doc framework, we conducted the six-phase protocol described in Section \ref{sec:protocol}.

% --- FASE 1: LOSS FUNCTION ---
\subsubsection{Objective Function Benchmarking}
The choice of the objective function is critical for shaping the metric space. Table \ref{tab:ablation-loss} details the performance of various loss families evaluated within our framework.

\begin{table}[h]
    \centering
    \caption{\textbf{Results (Loss Ablation):} Comparison of different metric learning objectives trained with the CaVL-Doc architecture. We observe that the optimal loss is dataset-dependent.}
    \label{tab:ablation-loss}
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Objective Function}} & \multicolumn{2}{c}{\textbf{Avg. EER (\%)}} \\
    \cmidrule(lr){2-3}
     & \textbf{LA-CDIP} & \textbf{RVL-CDIP} \\
    \midrule
    \textit{Euclidean} \\
    Contrastive Loss & 2.58 & 30.12 \\
    Triplet Loss & \textbf{1.48} & \textbf{26.28} \\
    \addlinespace
    \textit{Angular Margin} \\
    ArcFace & 3.41 & 29.39 \\
    CosFace & 3.97 & 28.86 \\
    % ExpFace & 4.59 & 28.50 \\
    % Circle Loss & 3.73 & 32.74 \\
    % Subcenter ArcFace & 3.15 & 28.74 \\
    \bottomrule
    \end{tabular}
\end{table}

\paragraph{Analysis} 
The results reveal a divergence based on domain complexity. On \textbf{LA-CDIP}, which focuses on structural layout variance, the \textbf{Triplet Loss} achieved the best performance (1.48\% EER). Similarly, on \textbf{RVL-CDIP}, using \textbf{Triplet Loss} also proved superior (26.28\% EER), likely due to its ability to model relative distances more effectively in a crowded embedding space. 

Based on this, we select \textbf{Triplet Loss} as the engine for both LA-CDIP and RVL-CDIP in the final comparison.



% --- FASE 2: PROFESSOR (SAMPLER) ---
\subsubsection{Sampling Strategy (The "Professor")}
Using the best loss from the previous section, we assessed the contribution of the RL-based "Professor" agent against a standard Random Sampler baseline.

\begin{table}[h]
    \centering
    \caption{\textbf{Results:} Impact of the Active Hard Mining strategy (Professor Agent) versus uniform random sampling.}
    \label{tab:ablation-professor}
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Sampling Strategy}} & \multicolumn{2}{c}{\textbf{Avg. EER (\%)}} \\
    \cmidrule(lr){2-3}
     & \textbf{LA-CDIP} & \textbf{RVL-CDIP} \\
    \midrule
    Random Sampling & -- & -- \\
    \textbf{Professor Agent} & \textbf{--} & \textbf{--} \\
    \bottomrule
    \end{tabular}
\end{table}

The active selection of hard negatives yielded significant improvements, particularly on the more challenging RVL-CDIP dataset, confirming that informative pairs are critical for convergence in noisy environments.

% --- FASE 3: CAPACITY (Q) ---
\subsubsection{Architecture Capacity ($Q$)}
We determined the optimal information bottleneck for the Multi-Query Attention mechanism comparing the single-query baseline against multi-query variations.

\begin{table}[h]
    \centering
    \caption{\textbf{Results:} Evaluation of the number of attention queries ($Q$). $Q=1$ represents the global attention baseline.}
    \label{tab:ablation-capacity}
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Queries ($Q$)}} & \multicolumn{2}{c}{\textbf{Avg. EER (\%)}} \\
    \cmidrule(lr){2-3}
     & \textbf{LA-CDIP} & \textbf{RVL-CDIP} \\
    \midrule
    $Q = 1$ & -- & -- \\
    $Q = 4$ & -- & -- \\
    $Q = 8$ & -- & -- \\
    \bottomrule
    \end{tabular}
\end{table}

The experiment reveals that $Q=\textbf{[X]}$ minimizes the EER consistently. While $Q=1$ captures global context, increasing to $Q=4$ allows the model to attend to distinct semantic regions simultaneously without overfitting.

% --- FASE 4: GEOMETRY ---
\subsubsection{Geometric Constraints}
With the architecture fixed, we fine-tuned the geometric properties in three steps: optimizing margin magnitude, exploring sub-centers, and testing elasticity.

\begin{table}[h]
    \centering
    \caption{\textbf{Results:} Step-wise optimization of geometric constraints. We evaluate Margin Magnitude ($m$), Sub-centers ($k$), and Stochasticity (Elasticity).}
    \label{tab:ablation-geometry}
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Configuration}} & \multicolumn{2}{c}{\textbf{Avg. EER (\%)}} \\
    \cmidrule(lr){2-3}
     & \textbf{LA-CDIP} & \textbf{RVL-CDIP} \\
    \midrule
    \multicolumn{3}{l}{\textit{1. Margin Magnitude}} \\
    Margin $m = 0.35$ & -- & -- \\
    Margin $m = 0.45$ & -- & -- \\
    Margin $m = 0.55$ & -- & -- \\
    \addlinespace
    \multicolumn{3}{l}{\textit{2. Sub-center}} \\
    $k = 1$ (Standard Baseline) & -- & -- \\
    $k = 3$ (Multi-center) & -- & -- \\
    $k = 5$ (Multi-center) & -- & -- \\
    \addlinespace
    \multicolumn{3}{l}{\textit{3. Robustness Analysis}} \\
    Static Margin ($\sigma = 0$) & -- & -- \\
    \textbf{Elastic Margin} & \textbf{--} & \textbf{--} \\
    \bottomrule
    \end{tabular}
\end{table}

Results demonstrate that while increasing class centers ($k>1$) helps with layout variance (LA-CDIP), the introduction of \textbf{Elasticity} provides the most universal gain in robustness, preventing overfitting to the few-shot support set across both datasets.

% --- FASE 5: SAMPLE SIZE ---
\subsubsection{Sample Size Analysis}
We evaluated the impact of training set size on model performance to assess data efficiency.

\begin{table}[h]
    \centering
    \caption{\textbf{Results:} Performance vs. Training Set Size (Number of Documents).}
    \label{tab:ablation-sample-size}
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Training Size}} & \multicolumn{2}{c}{\textbf{Avg. EER (\%)}} \\
    \cmidrule(lr){2-3}
     & \textbf{LA-CDIP} & \textbf{RVL-CDIP} \\
    \midrule
    $N = 2,500$ & -- & -- \\
    $N = 5,000$ & -- & -- \\
    $N = 10,000$ & -- & -- \\
    \bottomrule
    \end{tabular}
\end{table}

The results indicate that our framework achieves competitive performance with as few as 2,500 documents. While scaling to 5,000 and 10,000 samples yields improvements, the gains diminish, highlighting the few-shot efficiency of the CaVL-Doc adaptation.

% --- FASE 6: CURRICULUM ---
\subsubsection{Curriculum Learning Strategy}
Finally, Table \ref{tab:ablation-curriculum} validates the effectiveness of the two-stage training curriculum.

\begin{table}[h]
    \centering
    \caption{\textbf{Results:} Comparison of training schedules. The proposed curriculum combines global organization (Triplet) with local refinement (Angular).}
    \label{tab:ablation-curriculum}
    \begin{tabular}{lcc}
    \toprule
    \multirow{2}{*}{\textbf{Training Schedule}} & \multicolumn{2}{c}{\textbf{Avg. EER (\%)}} \\
    \cmidrule(lr){2-3}
     & \textbf{LA-CDIP} & \textbf{RVL-CDIP} \\
    \midrule
    Direct Training & -- & -- \\
    Triplet Only & -- & -- \\
    \textbf{Full Curriculum} & \textbf{--} & \textbf{--} \\
    \bottomrule
    \end{tabular}
\end{table}

The hybrid curriculum strategy achieved the lowest overall EER, validating the hypothesis that establishing global structure first enables more precise local boundary refinement.
% --- TABELAS DE RESULTADOS (INSERIDAS AQUI PARA POSICIONAMENTO NO TOPO) ---

\begin{table*}[t] 
\centering
\begin{threeparttable}
\caption{\textbf{Main Results Comparison.} Performance (EER \%) on LA-CDIP and RVL-CDIP datasets. We compare standard baselines, large-scale SOTA models, and our proposed CaVL-Doc framework using its optimal configuration for each domain.}
\label{tab:main-results}
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}Xlrr} 
\toprule
\multirow{2}{*}{\textbf{Method / Component}} & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{2}{c}{\textbf{EER (\%)}} \\
\cmidrule(lr){3-4}
 & & \textbf{LA-CDIP} & \textbf{RVL-CDIP} \\
\midrule

% --- Bloco 1: Naïve & Visual Baselines ---
\multicolumn{4}{l}{\textit{Naïve \& Visual Baselines}} \\
Pixel-Baseline (Reference)\tnote{1} & Cosine & 9.07 & 36.30 \\
ResNet-34 (VDM Embedding)\tnote{1} & Cosine & 4.13 & -- \\
\addlinespace

% --- Bloco 2: SOTA (Large Models) ---
\multicolumn{4}{l}{\textit{State-of-the-Art (Large Models \& ZSL Protocols)}} \\
Qwen-VL 2.5 (7B)\tnote{1} & Prompt & 6.61 & -- \\
InternVL3-14B\tnote{2} & Prompt & 2.85 & -- \\
ChatGPT-4o (Proprietary)\tnote{1} & Prompt & 2.75 & -- \\
GPT-4-Vision (ZSL Prompt)\tnote{3} & Prompt & -- & 30.10 \\
CICA (ZSL Split A)\tnote{3} & N/A & -- & 29.36 \\
\addlinespace

% --- Bloco 3: Base Model (Sem treino) ---
\multicolumn{4}{l}{\textit{Base Model (No Adaptation)}} \\
InternVL3-2B (Zero-Shot) & Prompt\tnote{2} & 38.98 & -- \\
InternVL3-2B (Zero-Shot) & Cosine & 5.86 & 36.70 \\
InternVL3-2B (Zero-Shot) & Euclidean & 3.57 & 34.80 \\
\addlinespace

% --- Bloco 4: OURS ---
\multicolumn{4}{l}{\textit{Ours (Proposed Framework)}} \\
\textbf{CaVL-Doc (Best Configuration)} & \textbf{Embedded} & \textbf{1.48} & \textbf{26.28} \\
\bottomrule
\end{tabularx}

\begin{tablenotes}
  \small
  \item[1] Result extracted from Macedo et al. \cite{macedo2025vdm}.
  \item[2] 'Prompt-Based' result extracted from Macedo et al. \cite{macedo2025vdm}.
  \item[3] EER is proxied as (100\% - Top-1 Accuracy) for reference comparison. Results from Scius-Bertrand et al. \cite{scius2024zeroshot} and Sinha et al. \cite{sinha2024cica}.
\end{tablenotes}

\end{threeparttable}
\end{table*}

% TABELA 3: ONE-SHOT ACCURACY
\begin{table*}[t] 
\centering
\begin{threeparttable}
\caption{One-Shot Top-1 Classification Accuracy (\%) on the \textbf{RVL-CDIP} dataset. This evaluates a one-shot (1:N) classification task using the \textbf{ZSL/GZSL Split A}\tnote{1} protocol.}
\label{tab:results-rvl-cdip-accuracy}
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}Xrrr} 
\toprule
\textbf{Method} & \textbf{Unseen Acc. \%} & \textbf{Seen Acc. \%}\tnote{2} & \textbf{H-Mean \%} \\
\midrule
CICA (Baseline) \cite{sinha2024cica} & 61.84 & 69.36 & 65.38 \\
\addlinespace[2pt]
\textbf{Ours (CaVL-Doc)} & \textbf{--} & \textbf{--} & \textbf{--} \\
\bottomrule
\end{tabularx}
\begin{tablenotes}
  \small 
  \item[1] \textbf{Split A (Unseen Classes):} email, form, handwritten, letter \cite{sinha2024cica}. \textbf{Seen Classes:} The remaining 12 classes of RVL-CDIP.
\end{tablenotes}
\end{threeparttable}
\end{table*}

\subsection{Efficiency vs. Performance}
Figure \ref{fig:param-plot} illustrates the trade-off between model size and performance. Our adapted 2B parameter model occupies the "sweet spot" in the bottom-left quadrant (Low Error, Low Parameters), significantly outperforming the 14B and proprietary models that reside in the high-parameter region. This demonstrates that CaVL-Doc is a viable solution for sovereign, on-premise deployment where computational resources are constrained.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{../../results/plots/LA-CDIP_performance_vs_parameters.png}
    \caption{Performance (EER \%) vs. Model Parameters (Log Scale) on the LA-CDIP dataset. Lower EER (y-axis) is better. Our final CaVL-Doc-adapted model achieves the best performance while remaining in the low-parameter (high-efficiency) quadrant.}
    \label{fig:param-plot}
\end{figure*}

\section{Conclusion}\label{sec:conclusion}

In this paper, we proposed CaVL-Doc, a novel framework for efficient Few-Shot Document Classification. We demonstrated that by leveraging the aligned multimodal tokens of a fixed LVLM (InternVL3-2B) and applying a specialized architecture with robust Elastic Margin losses, we can achieve state-of-the-art performance.

\subsection{Summary of Findings}

Our primary contributions are validated by the empirical results on the LA-CDIP dataset:
\begin{itemize}
    \item \textbf{Architectural Efficiency:} We showed that Multi-Query Attention Pooling is superior to standard mean pooling for capturing document semantics.
    
    \item \textbf{Robustness of Elastic Losses:} We demonstrated that "Elastic" angular margin losses (specifically ElasticArcFace) significantly outperform standard contrastive and static margin losses in few-shot scenarios, providing the necessary regularization to prevent overfitting.
    
    \item \textbf{SOTA Performance:} Our 2B parameter model, adapted with CaVL-Doc, outperforms proprietary models like ChatGPT-4o, proving that specialized adaptation is a viable alternative to massive model scaling.
\end{itemize}

\subsection{Future Work}

While our results are promising, this methodology opens several avenues for future research:
\begin{itemize}
    \item \textbf{Adaptive Margin Distributions:} Currently, the parameters of the elastic margin distribution ($\mu, \sigma$) are fixed. Future work could explore learning these parameters dynamically based on class difficulty.
    
    \item \textbf{Hierarchical Attention:} Extending the Multi-Query Attention to a hierarchical structure could allow the model to capture document structure at multiple levels of granularity (e.g., word, line, paragraph).
\end{itemize}

\section*{Declarations}

\begin{itemize}
\item \textbf{Funding} \\
Not applicable.

\item \textbf{Conflict of interest/Competing interests} \\
The authors declare they have no conflicts of interest.

\item \textbf{Ethics approval and consent to participate} \\
Not applicable. This study involves no human participants or animals.

\item \textbf{Consent for publication} \\
Not applicable.

\item \textbf{Data availability} \\
The datasets analyzed during this study, LA-CDIP and RVL-CDIP, are publicly available and were sourced from the authors of \cite{macedo2026vdm}.

\item \textbf{Materials availability} \\
Not applicable.

\item \textbf{Code availability} \\
The source code for the framework and experiments described in this study is available at [GitHub Repository Link, to be added upon publication].

\end{itemize}

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}

% \begin{appendices}

% \section{Additional Results}
% \label{appendix:results}

% \begin{table*}[ht]
% \caption{Comparative performance between different visual backbones and Large Language Models. Following the columns: the architecture name; the architecture edition, if exists; cross-validation over the \gls{ZSL} scenario; cross-validation over the \gls{GZSL} scenario; test performance on the \gls{ZSL} scenario; and test performance over the \gls{GZSL} scenario. Every value is a mean EER (\%) value over the \gls{CV} folds.}
% \label{tab:res}
% \centering
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \setlength{\tabcolsep}{6pt} % Adjust column spacing
% \begin{tabular}{lclcccc}
% \toprule
% Architecture                            & Edition & Params & \gls{ZSL}    & \gls{GZSL}    & Test \gls{ZSL} & Test \gls{GZSL} \\
% \midrule
% AlexNet                                 &         &  57M   & 8.92   & 5.45    & 17.33    & 6.31      \\ \midrule
% \multirow{4}{*}{VGG}                    & 11      &  129M  & 7.47   & 5.01    & 14.24    & 3.95      \\
%                                         & 13      &  129M  & 7.03   & 4.79    & 9.30     & 3.95      \\
%                                         & 16      &  134M  & 8.29   & 5.23    & 14.74    & 4.82      \\
%                                         & 19      &  139M  & 7.30   & 4.57    & 17.08    & 3.90      \\ \midrule
% \multirow{5}{*}{ResNet}                 & 18      &  11M   & 5.03   & 1.54    & 4.98     & 1.51      \\
%                                         & 34      &  21M   & 4.32   & 2.10    & 4.13     & 1.53      \\
%                                         & 50      &  23M   & 6.90   & 3.39    & 10.34    & 2.21      \\
%                                         & 101     &  42M   & 8.20   & 2.72    & 11.31    & 1.98      \\
%                                         & 152     &  58M   & 9.44   & 3.38    & 12.70    & 2.39      \\ \midrule
% \multirow{2}{*}{MobileNetV3}            & Small   &  1M    & 7.98   & 5.06    & 12.74    & 5.26      \\
%                                         & Large   &  4M    & 8.16   & 4.27    & 8.45     & 4.43      \\ \midrule
% \multirow{4}{*}{EfficientNet}           & 0       &  4M    & 4.41   & 2.27    & 6.02     & 0.95      \\
%                                         & 1       &  6M    & 3.93   & 3.54    & 8.88     & 2.70      \\
%                                         & 2       &  7M    & 5.73   & 2.61    & 7.29     & 2.14      \\
%                                         & 3       &  10M   & 5.65   & 3.64    & 7.37     & 2.34      \\ \midrule
% \multirow{2}{*}{\gls{ViT}}              & Base    &  87M   & 12.43  & 7.97    & 19.72    & 5.19      \\
%                                         & Large   &  305M  & 13.16  & 7.57    & 19.88    & 5.26      \\ \midrule
% Llama                                   & 3.2     & 11B    & --     & --      & 13.95    & 21.90     \\
% Qwen-VL                                 & 2.5     & 7B     & --     & --      & 6.61     & 4.20      \\
% InternVL                                & 2.5     & 8B     & --     & --      & 8.58     & 10.40     \\
%                                & 3      & 2B     & --     & --      & 38.98    & --        \\
%                                         & 3      & 8B     & --     & --      & 4.04     & --        \\
%                                         & 3     & 14B     & --     & --      & 2.85     & --        \\ \midrule
% \multirow{3}{*}{Siamese InternVL3}      & 1B      & 2B     & --     & --      & 4.83     & --        \\
%                                         & 2B      & 4B     & --     & --      & 2.79     & --        \\
%                                         & 8B      & 16B     & --     & --      & 3.34     & --        \\ \midrule
% MSLA-ZSL                                &         & 4B     & --     & --      & \textbf{1.94} & --    \\ \midrule
% GPT 4o mini                             & 2024-07-18 & *   & --     & --      & 4.70     & 4.07      \\
% GPT 4o                                  & 2024-11-20 & *   & --     & --      & 2.75     & 1.33      \\
% \bottomrule
% \end{tabular}
% \smallskip
% \parbox[t]{\textwidth}{\footnotesize
%     * The parameter count of GPT-4o has not been publicly disclosed.}
% \end{table*}


% \section{Section title of first appendix}\label{secA1}

% An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%
\bibliography{sn-bibliography}
%% if required, the content of .bbl file can be included here once bbl is generated
% \input sn-article.bbl

\end{document}
