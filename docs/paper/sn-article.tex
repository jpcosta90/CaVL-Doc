%Version 3.1 December 2024
% See section 11 of the User Manual for version history
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\documentclass[referee,sn-basic]{sn-jnl}% referee option is meant for double line spacing

%%=======================================================%%
%% to print line numbers in the margin use lineno option %%
%%=======================================================%%

%%\documentclass[lineno,pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%=========================================================================================%%
%% the documentclass is set to pdflatex as default. You can delete it if not appropriate.  %%
%%=========================================================================================%%

%%\documentclass[sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style

%%Note: the following reference styles support Namedate and Numbered referencing. By default the style follows the most common style. To switch between the options you can add or remove Numbered in the optional parenthesis. 
%%The option is available for: sn-basic.bst, sn-chicago.bst%  
 
%%\documentclass[pdflatex,sn-nature]{sn-jnl}% Style for submissions to Nature Portfolio journals
%%\documentclass[pdflatex,sn-basic]{sn-jnl}% Basic Springer Nature Reference Style/Chemistry Reference Style
\documentclass[pdflatex,sn-mathphys-num, iicol]{sn-jnl}% Math and Physical Sciences Numbered Reference Style
\usepackage[numbers]{natbib}
\usepackage{threeparttable}
%%\documentclass[pdflatex,sn-mathphys-ay]{sn-jnl}% Math and Physical Sciences Author Year Reference Style
%%\documentclass[pdflatex,sn-aps]{sn-jnl}% American Physical Society (APS) Reference Style
%%\documentclass[pdflatex,sn-vancouver-num]{sn-jnl}% Vancouver Numbered Reference Style
%%\documentclass[pdflatex,sn-vancouver-ay]{sn-jnl}% Vancouver Author Year Reference Style
%%\documentclass[pdflatex,sn-apa]{sn-jnl}% APA Reference Style
%%\documentclass[pdflatex,sn-chicago]{sn-jnl}% Chicago-based Humanities Reference Style

%%%% Standard Packages
%%<additional latex packages if required can be included here>
\RequirePackage{amsmath}
%
\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{wrapfig}

\usepackage{graphicx}%
\usepackage{multirow}%
\usepackage{amsmath,amssymb,amsfonts}%
\usepackage{amsthm}%
\usepackage{mathrsfs}%
\usepackage[title]{appendix}%
\usepackage{xcolor}%
\usepackage{textcomp}%
\usepackage{manyfoot}%
\usepackage{booktabs}%
\usepackage{algorithm}%
\usepackage{algorithmicx}%
\usepackage{algpseudocode}%
\usepackage{listings}%
\usepackage{tabularx} % Para criar tabelas com largura definida
\usepackage{array}    % Para alinhamento customizado (necessário para o \raggedright)
%%%%

%%%%%=============================================================================%%%%
%%%%  Remarks: This template is provided to aid authors with the preparation
%%%%  of original research articles intended for submission to journals published 
%%%%  by Springer Nature. The guidance has been prepared in partnership with 
%%%%  production teams to conform to Springer Nature technical requirements. 
%%%%  Editorial and presentation requirements differ among journal portfolios and 
%%%%  research disciplines. You may find sections in this template are irrelevant 
%%%%  to your work and are empowered to omit any such section if allowed by the 
%%%%  journal you intend to submit to. The submission guidelines and policies 
%%%%  of the journal take precedence. A detailed User Manual is available in the 
%%%%  template package for technical guidance.
%%%%%=============================================================================%%%%


\usepackage{glossaries}
\usepackage{glossaries-extra}
\setabbreviationstyle[acronym]{long-short}

\newacronym{AI}{AI}{Artificial Intelligence}
\newacronym{CV}{CV}{Cross-Validation}
\newacronym{DL}{DL}{Deep Learning}
\newacronym{EER}{EER}{Equal Error Rate}
\newacronym{ZSL}{ZSL}{Zero-Shot Learning}
\newacronym{GZSL}{GZSL}{Generalized Zero-Shot Learning}
\newacronym{LLM}{LLM}{Large Language Model}
\newacronym{LVLM}{LVLM}{Large Vision-Language Model}
\newacronym{OCR}{OCR}{Optical Character Recognition}
\newacronym{VDM}{VDM}{Visual Document Matching}
\newacronym{LO-CDIP}{LA-CDIP}{Layout-Aware Complex Document Information Processing}
\newacronym{LA-ZSL}{LA-ZSL}{Layout-Aware Zero-Shot Learning}
\newacronym{VLR}{VLR}{Visual Layout Recognition}
\newacronym{SGD}{SGD}{Stochastic Gradient Descent}
\newacronym{CDP}{CDP}{Copy Detection Patterns}
\newacronym{DLA}{DLA}{Document Layout Analysis}
\newacronym{ViT}{ViT}{Vision Transformer}
\newacronym{DLR}{DLR}{Document Layout Recogition}
\newacronym{FAR}{FAR}{False Acceptance Rate}
\newacronym{FRR}{FRR}{False Rejection Rate}
\newacronym{DIC}{DIC}{Document Image Classification}
\newacronym{ZS-DIC}{ZS-DIC}{Zero-Shot Document Classification}
\newacronym{FS-DIC}{FS-DIC}{Few-Shot Document Classification}

%% as per the requirement new theorem styles can be included as shown below
\theoremstyle{thmstyleone}%
\newtheorem{theorem}{Theorem}%  meant for continuous numbers
%%\newtheorem{theorem}{Theorem}[section]% meant for sectionwise numbers
%% optional argument [theorem] produces theorem numbering sequence instead of independent numbers for Proposition
\newtheorem{proposition}[theorem]{Proposition}% 
%%\newtheorem{proposition}{Proposition}% to get separate numbers for theorem and proposition etc.

\theoremstyle{thmstyletwo}%
\newtheorem{example}{Example}%
\newtheorem{remark}{Remark}%

\theoremstyle{thmstylethree}%
\newtheorem{definition}{Definition}%

\raggedbottom
%%\unnumbered% uncomment this for unnumbered level heads

\begin{document}

\title[Article Title]{CaVL-Doc: Comparative Aligned Vision-Language Document Embeddings for Zero-Shot DIC}

%%=============================================================%%
%% GivenName	-> \fnm{Joergen W.}
%% Particle	-> \spfx{van der} -> surname prefix
%% FamilyName	-> \sur{Ploeg}
%% Suffix	-> \sfx{IV}
%% \author*[1,2]{\fnm{Joergen W.} \spfx{van der} \sur{Ploeg} 
%%  \sfx{IV}}\email{iauthor@gmail.com}
%%=============================================================%%

% \author*[1,2]{\fnm{First} \sur{Author}}\email{iauthor@gmail.com}

% \author[2,3]{\fnm{Second} \sur{Author}}\email{iiauthor@gmail.com}
% \equalcont{These authors contributed equally to this work.}

% \author[1,2]{\fnm{Third} \sur{Author}}\email{iiiauthor@gmail.com}
% \equalcont{These authors contributed equally to this work.}

% \affil*[1]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{100190}, \state{State}, \country{Country}}}

% \affil[2]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{10587}, \state{State}, \country{Country}}}

% \affil[3]{\orgdiv{Department}, \orgname{Organization}, \orgaddress{\street{Street}, \city{City}, \postcode{610101}, \state{State}, \country{Country}}}

%%==================================%%
%% Sample for unstructured abstract %%
%%==================================%%

\abstract{
Large Vision-Language Models (LVLMs) have demonstrated impressive zero-shot capabilities in document understanding. However, their direct application to specialized, high-stakes enterprise classification tasks is often limited by performance saturation and the high computational cost of full fine-tuning. In this work, we introduce CaVL-Doc (Comparative Aligned Vision-Language Document Embeddings), a lightweight adaptation framework that leverages the robust multimodal alignment of pre-trained LVLMs without retraining the backbone. We propose a specialized architecture that processes aligned multimodal tokens using Multi-Query Attention Pooling and a Residual Projection Head. Furthermore, we address the challenge of intra-class variance in document images through a robust two-phase curriculum learning strategy. We systematically evaluate "Elastic" variants of angular margin losses (ElasticArcFace, ElasticCosFace, ElasticCircle) within this framework, introducing stochastic margins and hard negative mining to prevent overfitting and encourage a more generalizable metric space. Extensive experiments on the LA-CDIP and RVL-CDIP datasets demonstrate that our approach, applied to a 2B parameter model, achieves state-of-the-art performance, significantly outperforming larger proprietary models while maintaining high inference efficiency.
}

%%================================%%
%% Sample for structured abstract %%
%%================================%%

% \abstract{\textbf{Purpose:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Methods:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Results:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.
% 
% \textbf{Conclusion:} The abstract serves both as a general introduction to the topic and as a brief, non-technical summary of the main results and their implications. The abstract must not include subheadings (unless expressly permitted in the journal's Instructions to Authors), equations or citations. As a guide the abstract should not exceed 200 words. Most journals do not set a hard limit however authors are advised to check the author instructions for the journal they are submitting to.}

\keywords{Few-Shot Document Image Classification, Metric Learning, Curriculum Learning, LVLM, Embedding Space Learning}

%%\pacs[JEL Classification]{D8, H51}

%%\pacs[MSC Classification]{35A01, 65L10, 65L12, 65L20, 65L70}

\maketitle

\section{Introduction}\label{sec:introduction}

% --- Início da sua Introdução ---

The field of Document Understanding encompasses the analysis of content and structure in documents across various formats and modalities, such as text, images, tables, and graphics \cite{formUnderstandingSurvey}. Within this spectrum, accurate \glsfirst{DIC} is crucial for organizations to ensure compliance and maintain consistency in diverse applications, making document classification an extensively studied task. The complexity of this task is accentuated by the dynamic nature of real-world documents; forms change, new types of documents are introduced, and traditional classification models often prove insufficient, requiring frequent and costly retraining to remain relevant \cite{macedo2025vdm}.

\gls{DIC} has seen an evolution from structure-based methods to visual-based methods and, more recently, to hybrid approaches that combine textual and visual features. However, a comprehensive review of the field points to open issues, including the critical need to learn from few or zero training samples, with \glsfirst{ZS-DIC} (Zero-Shot Document Classification) and \glsfirst{FS-DIC} (Few-Shot Document Classification) emerging as promising directions to address this data scarcity challenge \cite{macedo2025vdm,voerman2025optimizing,bakkali2025globaldoc}.

The advent of \glsfirst{LLM} and, more specifically, multimodal \glsfirst{LVLM} (Large Vision-Language Models), has provided a powerful mechanism for achieving \gls{ZS-DIC}. Studies show that these models can achieve competitive performance with minimal labeled data by leveraging their vast pre-training knowledge through simple textual prompts \cite{scius2024zeroshot}. This shifts the focus from model training to efficient model adaptation.

Despite this powerful zero-shot capability, two critical challenges limit the direct applicability of LVLMs in high-volume, enterprise-grade pipelines:

\begin{enumerate}
    \item \textbf{Performance Saturation and Refinement:} While initial ZS performance is competitive, achieving the reliability required for critical business processes demands robust adaptation to complex, domain-specific visual nuances. Simple prompt adjustments or standard fine-tuning with few samples often fall short of capturing this nuance \cite{scius2024zeroshot}.
    \item \textbf{Operational Sovereignty and Cost:} The rapidly growing compute demand for inference in large proprietary models raises concerns about energy footprint, financial sustainability \cite{patel2024characterizing, cruz2025innovating, patterson2022carbon}, and, more importantly, data privacy and technological autonomy. Regulatory challenges demand solutions that can be deployed on-premise \cite{wiest2024privacy, strong2025trustworthy}, creating a clear demand for efficient, open-source foundation models that can be sovereignly adapted \cite{floridi2025open}.
\end{enumerate}

This reveals a critical gap in the literature: a lack of methodologies for efficient and robust few-shot adaptation of LVLMs. The state-of-the-art for efficient FSL in documents is converging on Metric Learning—such as Prototypical Networks \cite{voerman2025optimizing, bakkali2025globaldoc} or Siamese Networks \cite{macedo2025vdm}—which learn a specialized embedding space on top of fixed model features. However, these standard metric learning approaches treat all training samples equally. They often fail when faced with high intra-class variance (e.g., visually distinct documents in the same class) and hard-to-distinguish negative pairs, leading to sub-optimal generalization and accuracy issues.

To address this gap, we introduce CaVL-Doc (Comparative Aligned Vision-Language Document Embeddings), a novel framework for efficient few-shot adaptation. Our approach enhances standard metric learning by leveraging the rich, aligned multimodal tokens of modern LVLMs. Instead of treating the model as a black-box feature extractor, we design a specialized architecture that aggregates these tokens using Multi-Query Attention Pooling. Crucially, to handle the high variance of document layouts, we move beyond standard contrastive losses. We implement and evaluate a family of "Elastic" angular margin losses (e.g., ElasticArcFace, ElasticCircle). These losses introduce a stochastic margin during training, which prevents the model from overfitting to the limited support set and forces the learning of a more robust and generalizable embedding space. We demonstrate that CaVL-Doc, applied to a 2B parameter open-source LVLM, achieves state-of-the-art few-shot performance, significantly outperforming standard metric learning baselines.

% --- FIGURA CORRIGIDA PARA LAYOUT DE 2 COLUNAS ---
\begin{figure}[t]
\centering
% MUDANÇA: 'textwidth' foi substituído por 'columnwidth'
% 'columnwidth' se ajusta automaticamente à largura de uma única coluna.
\includegraphics[width=\columnwidth]{../assets/problema pesquisa DAML.png}
\caption{The conceptual problem of metric adaptation. An initial baseline metric is used for classification (green). The model's performance is evaluated (red), and the results feed an adaptation loop (purple) to continually improve the classification metric.}
\label{fig:daml-loop}
\end{figure}

\subsection{The main contributions}
The key contributions of this work are summarized as follows:
\begin{enumerate}
    \item The proposal of CaVL-Doc, a novel framework for efficient, few-shot adaptation of LVLMs in document image classification, which operates on fixed-backbone model embeddings.
    
    \item The design of a specialized Metric Learning Head that leverages aligned multimodal tokens from InternVL3, utilizing Multi-Query Attention Pooling to capture diverse semantic features.
    
    \item A systematic evaluation of "Elastic" angular margin losses (ElasticArcFace, ElasticCosFace, ElasticCircle) for document classification, demonstrating their superiority over static margin losses in few-shot scenarios.
    
    \item An empirical demonstration that CaVL-Doc significantly outperforms standard FSL metric learning baselines (e.g., Prototypical Networks and standard Contrastive Loss) on the standard LA-CDIP and RVL-CDIP document datasets.
\end{enumerate}

The remainder of this article is structured as follows: Section~\ref{sec:related-work} reviews the state-of-the-art in few-shot document classification and metric learning. Section~\ref{sec:methodology} details the proposed CaVL-Doc framework, including the LVLM token alignment, the Multi-Query Attention architecture, and the Elastic Margin Losses. Section~\ref{sec:experiments} describes the experimental setup, datasets, and baseline comparisons. Section~\ref{sec:results} presents and analyzes the empirical results of our framework. Finally, section \ref{sec:conclusion} concludes the article by summarizing our contributions and outlining future research directions.

% --- Fim da Introdução ---

\section{Related Work}\label{sec:related-work}
This work is positioned at the intersection of several key research areas within document analysis and machine learning. To provide a comprehensive background for our proposed CaVL-Doc framework, this section reviews the foundational and recent advancements in four critical domains:
(1) The evolution of Document Image Classification (DIC) toward Few-Shot (FSL) paradigms; 
(2) The application of Large Vision-Language Models (LVLMs) as fixed-backbone feature extractors; 
(3) The adoption of Metric Learning as the state-of-the-art for efficient FSL in documents; and 
(4) The evolution of loss functions for robust embedding learning.

\subsection{Document Classification: From ZS-DIC to Few-Shot Adaptation}\label{sec:rw_dic}
This section reviews the evolution of Document Image Classification (DIC), highlighting the transition from traditional supervised methods \cite{liu_document_2021} to the challenges of data-scarce environments. The advent of LVLMs initially established powerful baselines for Zero-Shot Document Classification (ZS-DIC) through simple prompting \cite{scius2024zeroshot}.

However, as noted in recent literature, ZS-DIC often saturates in performance and lacks the precision required for specialized enterprise tasks \cite{scius2024zeroshot}. This has shifted the research focus to the more practical challenge of Few-Shot Learning (FSL) \cite{bakkali2025globaldoc, voerman2025optimizing}. The goal of FSL is to efficiently adapt a model to new, unseen document classes using only a handful of examples. This scenario, which balances performance with the high cost of data annotation, is the primary focus of our work.

\subsection{Metric Learning for FSL Document Classification}\label{sec:rw_dml}
While full fine-tuning of LVLMs is one FSL approach \cite{scius2024zeroshot}, it remains computationally expensive. A more efficient and dominant strategy in the recent FSL document literature is Deep Metric Learning (DML) \cite{bakkali2025globaldoc, voerman2025optimizing, macedo2026vdm}.

DML aims to learn a discriminative embedding space—typically using a lightweight "projection head" over fixed LVLM features \cite{shu2024dsncl}—where similar samples are pulled closer and dissimilar samples are pushed apart \cite{shu2024dsncl, yan2024lmmetric}. The state-of-the-art in FSL for documents has converged on this approach. For instance, Voerman et al. conduct a comparative analysis for identity document classification and conclude that Prototypical Networks (a classic DML method) are the most practical and effective FSL solution \cite{voerman2025optimizing}. Similarly, Bakkali et al. define their FSL task using a Prototypical Network, which calculates a class centroid from the support set embeddings \cite{bakkali2025globaldoc}. Other works, such as Macedo et al., achieve the same goal using Siamese Networks with a standard Contrastive Loss \cite{macedo2025vdm}.

However, this reliance on standard DML methods exposes a critical gap: these techniques treat all training pairs equally. They struggle with high intra-class variance and complex negative pairs, leading to sub-optimal generalization and what Voerman et al. describe as a "precision issue" \cite{voerman2025optimizing}. Our work addresses this specific gap.

\subsection{Robust Loss Functions for Metric Learning}\label{sec:rw_cl}
The second axis of our framework focuses on optimizing the similarity metric itself. This is a common objective in Deep Metric Learning (DML), which aims to learn a discriminative embedding space where similar samples are pulled closer together and dissimilar samples are pushed far apart \cite{shu2024dsncl, yan2024lmmetric}. Classic DML objectives for retrieval tasks often rely on Contrastive Loss \cite{shu2024dsncl} or Triplet Loss \cite{yan2024lmmetric}.

However, these Euclidean-based losses often fail to enforce sufficient intra-class compactness. To address this, angular margin losses were introduced, such as ArcFace, CosFace, and Circle Loss. These losses project features onto a hypersphere and enforce an angular margin penalty, leading to more discriminative features. Recently, "Elastic" variants of these losses have been proposed to handle data with high noise or variance. By treating the margin as a random variable sampled from a distribution, rather than a fixed hyperparameter, these losses prevent the model from overfitting to easy samples or noisy labels, promoting a more robust decision boundary. Our work systematically applies and evaluates these elastic losses in the context of few-shot document classification.

% --- SEÇÃO DE METODOLOGIA REESCRITA ---

\section{The CaVL-Doc Framework}\label{sec:methodology}

Our framework is designed for efficient Few-Shot Document Classification (FSL), where system performance must be adapted to specialized enterprise domains using only a handful of examples, without requiring full model retraining.

The methodology is based on learning a specialized, lightweight metric head on top of a \textit{fixed} LVLM backbone. The core of our contribution is the CaVL-Doc approach, which enhances standard metric learning through two key innovations: (1) a specialized architecture that leverages aligned multimodal tokens via Multi-Query Attention Pooling, and (2) the use of "Elastic" angular margin losses to enforce a robust and generalizable embedding space.

\subsection{Multimodal Token Alignment with InternVL3}\label{sec:backbone}

The foundation of our framework is the InternVL3-2B model \cite{chen2024internvl}, a state-of-the-art Large Vision-Language Model (LVLM). Unlike traditional CNNs or ViTs that process images in isolation, InternVL3 is trained to align visual features with linguistic concepts.

Given an input document image $x$, the model processes it through a vision encoder and a cross-modal alignment stage. Instead of using the final pooled output (which compresses the entire document into a single vector), we extract the sequence of $N$ aligned multimodal tokens, $T = \{t_1, t_2, \dots, t_N\}$, from the last hidden layer of the language model component. These tokens represent rich, localized semantic features of the document (e.g., specific text blocks, layout elements, stamps) that are already aligned with the model's language understanding.

To ensure efficiency and prevent catastrophic forgetting of the pre-trained knowledge, the entire InternVL3 backbone remains \textbf{frozen} during training. Only the lightweight adaptation head described below is optimized.

\subsection{Architecture: Multi-Query Attention and Residual Head}\label{sec:architecture}

Standard metric learning approaches often use a simple Mean Pooling followed by a Linear Layer. We argue that this destroys the fine-grained semantic information present in the token sequence $T$. To address this, CaVL-Doc employs a specialized two-stage architecture.

\subsubsection{Multi-Query Attention Pooling}
To capture the diverse semantic aspects of a document, we introduce a Multi-Query Attention Pooling mechanism. Instead of condensing the document into a single vector, we define a set of $Q$ learnable query vectors, $Q = \{q_1, \dots, q_Q\}$.

For each query $q_j$, the mechanism computes an attention score over the input tokens $T$:
\begin{equation}
    \alpha_{j,i} = \text{softmax}\left(\frac{q_j \cdot t_i^T}{\sqrt{d}}\right)
\end{equation}
The resulting pooled vector $h_j$ is a weighted sum of the tokens: $h_j = \sum_{i=1}^{N} \alpha_{j,i} t_i$. This allows the model to learn distinct "views" of the document (e.g., one query might focus on header information, another on visual layout). These $Q$ vectors are then concatenated to form a comprehensive document representation.

\subsubsection{Residual Projection Head}
The aggregated features are then processed by a \textbf{Residual Projection Head} ($\mathcal{G}_\phi$). Standard Multi-Layer Perceptrons (MLPs) can sometimes distort the well-structured geometry of the pre-trained feature space. To mitigate this, we employ a residual connection:
\begin{equation}
    v' = v + \text{MLP}(v)
\end{equation}
where $v$ is the concatenated output of the attention pooling. This residual structure ensures that the original, robust LVLM features are preserved as a baseline, while the MLP learns only the necessary non-linear transformations to adapt the metric space to the specific document classification task.

\subsubsection{Efficiency: Once Learning vs. Autoregressive Decoding}
A critical advantage of this architecture is its alignment with the concept of "Once Learning" \cite{weigang1999study}. Drawing inspiration from biological neural processing, where recognition often occurs as a rapid, parallel integration of stimuli rather than a sequential reconstruction, this paradigm contrasts sharply with standard LVLM usage. In the standard approach, comparing documents involves prompting the model and waiting for it to generate a textual response via autoregressive token decoding. This process is inherently sequential, akin to a slow, deliberative reasoning chain.

In contrast, CaVL-Doc captures the entire semantic structure of the document—represented by the sequence of aligned multimodal tokens—in a single, holistic forward pass. This "at once" aggregation transforms the complex token sequence into a unified metric representation without the latency of sequential generation. The comparison is then reduced to a direct metric operation in the embedding space, which is orders of magnitude faster and scalable to large databases.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{../assets/CAVL_structure.png}
\caption{Overview of the CaVL-Doc framework. The architecture leverages aligned multimodal tokens from the frozen InternVL3 backbone. These tokens are aggregated via Multi-Query Attention Pooling and processed by a Residual Projection Head, which is trained using Elastic Margin Losses to ensure a robust embedding space.}
\label{fig:cavl-structure}
\end{figure}

\subsection{Robust Metric Learning with Elastic Losses}\label{sec:losses}

The final component of CaVL-Doc is the objective function used to train the projection head $\mathcal{G}_\phi$. Standard metric learning often relies on Contrastive or Triplet losses, which operate in Euclidean space. However, these losses can struggle to enforce compact intra-class clusters, especially with the high variance found in document images.

\subsubsection{Angular Margin Losses}
We adopt Angular Margin Losses (e.g., ArcFace, CosFace, Circle Loss), which project features onto a hypersphere and enforce a margin penalty in the angular space. For example, ArcFace adds an additive angular margin $m$ to the target angle $\theta_{y_i}$:
\begin{equation}
    \mathcal{L}_{Arc} = -\log \frac{e^{s \cdot \cos(\theta_{y_i} + m)}}{e^{s \cdot \cos(\theta_{y_i} + m)} + \sum_{j \neq y_i} e^{s \cdot \cos(\theta_{j})}}
\end{equation}
where $s$ is a scaling factor. This enforces a stricter decision boundary, pushing samples of the same class closer together in terms of cosine similarity.

\subsubsection{Elastic Margins for Few-Shot Robustness}
In Few-Shot Learning, a fixed margin $m$ can lead to overfitting, as the model may try to enforce a rigid boundary based on very few samples. To address this, we implement \textbf{Elastic Margin Losses} (ElasticArcFace, ElasticCosFace, ElasticCircle).

The core idea is to treat the margin $m$ not as a fixed hyperparameter, but as a random variable sampled from a Gaussian distribution at each training step:
\begin{equation}
    m \sim \mathcal{N}(\mu, \sigma^2)
\end{equation}
By introducing this stochasticity, the "Elastic" losses prevent the model from collapsing onto a specific, rigid boundary. It forces the network to learn a more flexible and robust embedding space that can accommodate the uncertainty inherent in few-shot data. We systematically evaluate the performance of ElasticArcFace, ElasticCosFace, ElasticCircle, and ElasticExpFace against their static counterparts.

\subsection{Optimization Strategy: A Hybrid Curriculum-RL Approach}\label{sec:curriculum}

To further enhance the generalization capability of our model, particularly for unseen classes, we propose a robust training strategy that combines a Macro-Level Curriculum (scheduling the loss function) with a Micro-Level RL Agent (scheduling the data distribution).

\subsubsection{Macro-Level: Three-Phase Loss Schedule}
Instead of training with a static objective throughout, we divide the optimization process into three distinct phases. This strategy is designed to first establish a stable geometric baseline and then refine the decision boundaries by progressively introducing angular constraints and elasticity.

\begin{enumerate}
    \item \textbf{Phase 1: Geometric Alignment (Contrastive Loss).} The primary objective of the first phase is to stabilize the latent space and form coarse class clusters. We employ a standard Contrastive Loss, which operates on pair-wise distances without enforcing angular margins. This allows the network to learn the global structure of the data manifold.
    
    \item \textbf{Phase 2: Angular Refinement (ExpFace Loss).} Once the global structure is established, we switch to an Angular Margin Loss (ExpFace). This phase enforces strict angular separability between classes, refining the decision boundaries.
    
    \item \textbf{Phase 3: Elastic Adaptation (Elastic ExpFace).} Finally, to handle hard samples and prevent overfitting, we introduce stochasticity via Elastic ExpFace. The margin becomes a random variable ($m \sim \mathcal{N}(\mu, \sigma^2)$), forcing the network to learn a "thicker" and more robust boundary that generalizes better to unseen classes.
\end{enumerate}

\subsubsection{Micro-Level: RL-Based Data Selection (The Professor)}
Crucially, unlike traditional curriculum learning that relies on fixed heuristics (e.g., "start with easy samples"), our framework employs a Reinforcement Learning agent (the "Professor") to actively select training data \textit{throughout all three phases}.

The Professor is a lightweight policy network that observes the current state of the Student model (represented by the loss distribution of a candidate batch) and selects the most informative pairs for training. This ensures that whether the Student is in the "Alignment" phase or the "Refinement" phase, it is always training on the optimal set of samples (e.g., hard negatives) required to maximize its learning progress at that specific moment. This hybrid approach combines the stability of a loss curriculum with the adaptability of RL-based hard mining.

% --- SEÇÃO DE EXPERIMENTOS REESCRITA ---

\section{Experimental Setup}\label{sec:experiments}

To validate our proposed \textbf{CaVL-Doc} framework, we conduct a series of experiments designed to measure the impact of our architectural choices and the effectiveness of the Elastic Margin losses. This section details the datasets, evaluation protocols, and implementation settings.

%------------------------------------------------
\subsection{Datasets and Evaluation Metrics}
%------------------------------------------------

We evaluate our framework on two standard, large-scale document classification benchmarks.

\subsubsection{Datasets: LA-CDIP and RVL-CDIP}
We use two public document image datasets for our experiments:
\begin{itemize}
    \item \textbf{LA-CDIP} \cite{macedo2025vdm}: This is our primary evaluation dataset. It is a reorganization of the RVL-CDIP database, comprising \textbf{4,993 documents across 144 classes}, specifically curated to emphasize visual structure over semantic information.
    
    \item \textbf{RVL-CDIP} \cite{harley2015rvlcdip}: A widely-used benchmark consisting of 400,000 document images across 16 classes. This dataset has been recently benchmarked and extended with standardized Zero-Shot Learning (ZSL) \cite{sinha2024cica} and Few-Shot Learning (FSL) \cite{scius2024zeroshot} protocols.
\end{itemize}

For both datasets, we follow a standard Few-Shot Learning protocol. We use the official training splits to train our metric learning head. We report all final performance on the official validation/test sets.

% --- Subseção de Métricas de Avaliação ---
\subsection{Evaluation Metrics}

We evaluate our framework using two distinct metrics to capture both the discriminative power of the embedding space and its practical classification utility.

\subsubsection{Pair-wise Verification (EER)}
% (Este é o seu texto original, que está ótimo)
Given our pair-wise matching setup, we evaluate performance using the Equal Error Rate (EER). The EER is the point on the Receiver Operating Characteristic (ROC) curve where the False Acceptance Rate (FAR) equals the False Rejection Rate (FRR). 

A lower EER indicates a more discriminative model, as it represents the lowest achievable error rate when the acceptance threshold is set to equalize false positives and false negatives. This metric is standard for zero-shot verification tasks as it provides a single, threshold-independent measure of the separability between positive pairs (same class) and negative pairs (different classes).

\subsubsection{One-Shot Classification Accuracy (Top-1 Acc.)}
% (Esta é a nova seção que você solicitou)
To measure the practical utility of the learned metric space for classification, we also report the Top-1 One-Shot Classification Accuracy. This protocol follows a standard Few-Shot Learning (FSL) setup.

For each of the $K$ classes in the test set, we randomly select one single image to serve as the class prototype (the "support" sample). The remaining images in the test set are then used as the "query" set. A query image $x_q$ is classified by finding the class $k$ whose prototype $v_k$ is closest in the learned metric space:
\begin{equation}
    \hat{c} = \underset{k \in \{1, \dots, K\}}{\arg\min} \mathcal{S}_{new}(\mathcal{G}(v_q; \phi), \mathcal{G}(v_k; \phi))
\end{equation}
Accuracy is the percentage of query images assigned to the correct class $\hat{c} = c_{true}$. We evaluate this in two settings, similar to Generalized Zero-Shot Learning (GZSL) protocols \cite{scius2024zeroshot, sinha2024cica}:
\begin{itemize}
    \item \textbf{FSL (Unseen Classes):} Classification accuracy on query images from \textit{unseen} classes, where the model must choose only from the $K_{unseen}$ class prototypes.
    \item \textbf{GFSL (Seen + Unseen Classes):} Classification accuracy on a mixed query set, where the model must choose from all $K_{seen} + K_{unseen}$ prototypes. This tests the model's ability to distinguish new classes without "forgetting" the original ones.
\end{itemize}
%------------------------------------------------
\subsection{Implementation Details and Hyperparameters}
%------------------------------------------------

All experiments are conducted using PyTorch on a system equipped with NVIDIA GPUs.

\begin{itemize}
    \item \textbf{LVLM Backbone}: We use the \texttt{InternVL3-2B} model as our frozen feature extractor $\mathcal{F}_\theta$. We extract the sequence of aligned multimodal tokens from the last hidden layer. The backbone model is loaded in 16-bit precision.
    
    \item \textbf{Metric Head Training:} The \texttt{ProjectionHead} $\mathcal{G}_\phi$ is trained using the AdamW optimizer with a learning rate of $1 \times 10^{-4}$ and a weight decay of $1 \times 10^{-4}$. The projection output dimension $m$ is set to 512. We use a batch size of 32 and train for 20 epochs.
    
    \item \textbf{Loss Functions:} For the Elastic losses, we set the margin distribution parameters as $\mu=0.5$ and $\sigma=0.05$. The scaling factor $s$ is set to 30.
\end{itemize}

%------------------------------------------------
\subsection{Baseline and Comparison Methods}
%------------------------------------------------

To validate the effectiveness of our full CaVL-Doc framework, we compare its performance against several key baselines:

\begin{itemize}
    \item \textbf{Pixel-Baseline:} A non-deep learning baseline that performs pair-wise comparison using raw pixel values with Euclidean distance, as reported in \cite{macedo2026vdm}.
    
    \item \textbf{Base-LVLM:} The initial few-shot performance of the frozen $\mathcal{F}_\theta$ (InternVL3-2B) using standard mean pooling and Cosine/Euclidean distance. This represents the system's performance without any specialized training.
    
    \item \textbf{Standard Metric Learning (Ablation):} To isolate the benefit of our architecture and losses, we train a standard MLP head with Contrastive Loss.
    
    \item \textbf{Ours (CaVL-Doc Framework):} The full framework using Multi-Query Attention Pooling and Elastic Margin Losses.
\end{itemize}

\section{Results and Discussion}\label{sec:results}

This section analyzes the empirical performance of our CaVL-Doc framework. We evaluate the contribution of our architectural choices and the robust loss functions by comparing performance against initial baselines and state-of-the-art (SOTA) models.

The summary of our results on the LA-CDIP dataset is presented in Table \ref{tab:results-la-cdip}, and the setup for future validation on RVL-CDIP is in Table \ref{tab:results-rvl-cdip}.

% --- TABELA 1: LA-CDIP (REESCRITA) ---
\begin{table*}[t] 
\centering
\begin{threeparttable}
\caption{Framework performance on the \textbf{LA-CDIP} dataset. Performance is measured by Equal Error Rate (EER). A lower EER indicates better performance. We compare standard baselines against our CaVL-Doc architecture trained with different loss functions.}
\label{tab:results-la-cdip}
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}Xlr} 
\toprule
\textbf{Method / Component} & \textbf{Metric} & \textbf{EER (\%)} \\
\midrule
% --- Bloco de Baselines ---
\multicolumn{3}{l}{\textit{Baselines}} \\
\addlinespace[2pt]
Pixel-Baseline (Reference)\tnote{1} & Cosine & 9.07 \\
ResNet-34\tnote{1} & Learned (VDM) & 4.13 \\
Qwen-VL 2.5\tnote{1} & Prompt-Based & 6.61 \\
ChatGPT-4o (SOTA Target)\tnote{1} & Prompt-Based & 2.75 \\
InternVL3-14B\tnote{2} & Prompt-Based & 2.85 \\
\addlinespace

% --- Bloco do Nosso Modelo ---
\multicolumn{3}{l}{\textit{Proposed Adaptation (on InternVL3-2B)}} \\
InternVL3-2B (Base Model)\tnote{2}  & Prompt-Based & 38.98 \\
InternVL3-2B (Base Model) & Cosine & 5.86 \\
InternVL3-2B (Base Model) & Euclidean & 3.57 \\
\addlinespace[2pt]
\multicolumn{3}{l}{\textit{CaVL-Doc (Multi-Query Attention + Residual Head)}} \\
Ours (w/ Contrastive Loss) & Learned & -- \\
Ours (w/ ArcFace) & Learned & -- \\
Ours (w/ ElasticArcFace) & Learned & \textbf{--} \\
Ours (w/ ElasticCosFace) & Learned & -- \\
Ours (w/ ElasticCircle) & Learned & -- \\
Ours (w/ ElasticExpFace) & Learned & -- \\
\textbf{Ours (CaVL-Doc w/ Curriculum)} & \textbf{Learned} & \textbf{--} \\
\bottomrule
\end{tabularx}

\begin{tablenotes}
  \small
  \item[1] Result extracted from Macedo et al.\cite{macedo2025vdm}.
  \item[2] 'Prompt-Based' result extracted from Macedo et al. \cite{macedo2025vdm}.
\end{tablenotes}

\end{threeparttable}
\end{table*}

% --- TABELA 2: RVL-CDIP (COMO SOLICITADO, EM BRANCO) ---
\begin{table*}[t] 
\centering
\begin{threeparttable}
\caption{Framework performance on the \textbf{RVL-CDIP} dataset. Performance is measured by Equal Error Rate (EER). A lower EER indicates better performance.}
\label{tab:results-rvl-cdip}
% --- MUDANÇA AQUI ---
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}Xlr} 
\toprule
\textbf{Method / Component} & \textbf{Metric} & \textbf{EER (\%)} \\
\midrule
% --- Bloco de Baselines ---
\multicolumn{3}{l}{\textit{Baselines (Reference \& SOTA)}} \\
\addlinespace[2pt] % Adiciona um pequeno espaço
Pixel-Baseline (Reference) & Cosine & 36.30 \\
\addlinespace[2pt]
% --- Baselines SOTA (com proxy EER) ---
GPT-4-Vision (ZSL Prompt)\tnote{1} \cite{scius2024zeroshot} & Prompt-Based & 30.10 \\
CICA (ZSL Split A T1)\tnote{1} \cite{sinha2024cica} & N/A & 29.36 \\
% Mistral-7B-Class (FSL 1.6k)\tnote{1} \cite{scius2024zeroshot} & N/A & 16.60 \\
% BERT+NasNet (Fully Trained)\tnote{1} \cite{scius2024zeroshot} & N/A & 2.90 \\
\addlinespace % Adiciona espaço para separar os blocos

% --- Bloco do Nosso Modelo ---
\multicolumn{3}{l}{\textit{Proposed Adaptation (on InternVL3-2B)}} \\
\addlinespace[2pt]
InternVL3-2B (Base Model) & Cosine & 36.70 \\
InternVL3-2B (Base Model) & Euclidean & 34.80 \\
\addlinespace[2pt]
\multicolumn{3}{l}{\textit{CaVL-Doc (Multi-Query Attention + Residual Head)}} \\
Ours (w/ Contrastive Loss) & Learned & -- \\
Ours (w/ ArcFace) & Learned & -- \\
Ours (w/ ElasticArcFace) & Learned & \textbf{--} \\
Ours (w/ ElasticCosFace) & Learned & -- \\
Ours (w/ ElasticCircle) & Learned & -- \\
Ours (w/ ElasticExpFace) & Learned & -- \\
\textbf{Ours (CaVL-Doc w/ Curriculum)} & \textbf{Learned} & \textbf{--} \\
\bottomrule
% --- MUDANÇA AQUI ---
\end{tabularx}

% --- Bloco de Notas da Tabela ---
\begin{tablenotes}
  \small % Deixa as notas menores
  \item[1] EER is proxied as (100\% - Top-1 Accuracy). These models were evaluated on a multi-class classification task, not pair-wise matching. Results from CICA \cite{sinha2024cica} (avg. ZSL T1 accuracy of 67.29\%) and Scius-Bertrand et al. \cite{scius2024zeroshot} (ZSL accuracy of 69.9\%, FSL of 83.4\%, Full of 97.1\%).
\end{tablenotes}

\end{threeparttable}
\end{table*}

% --- Tabela 3: Acurácia de Classificação One-Shot (RVL-CDIP Split A) ---
% Pacotes recomendados: \usepackage{threeparttable} \usepackage{tabularx} \usepackage{array}
\begin{table*}[t] 
\centering
% Inicia o ambiente para notas de tabela
\begin{threeparttable}
\caption{One-Shot Top-1 Classification Accuracy (\%) on the \textbf{RVL-CDIP} dataset. This evaluates a one-shot (1:N) classification task using the \textbf{ZSL/GZSL Split A}\tnote{1} protocol.}
\label{tab:results-rvl-cdip-accuracy}
% Use tabularx para largura consistente (Xrrr = 4 colunas)
\begin{tabularx}{\textwidth}{>{\raggedright\arraybackslash}Xrrr} 
\toprule
% Colunas para ZSL (Unseen), GZSL (Seen), e GZSL (Harmonic Mean)
\textbf{Method} & \textbf{Unseen Acc. \%} & \textbf{Seen Acc. \%}\tnote{2} & \textbf{H-Mean \%} \\
\midrule

% --- Bloco de Baselines SOTA ---
CICA (Baseline) \cite{sinha2024cica} & 61.84 & 69.36 & 65.38 \\
\addlinespace[2pt]

% --- Bloco do Nosso Modelo ---
\textbf{Ours (CaVL-Doc w/ ElasticArcFace)} & \textbf{--} & \textbf{--} & \textbf{--} \\
\bottomrule
\end{tabularx}

% --- Bloco de Notas da Tabela ---
\begin{tablenotes}
  \small % Deixa as notas menores
  \item[1] \textbf{Split A (Unseen Classes):} email, form, handwritten, letter \cite{sinha2024cica}. \textbf{Seen Classes:} As 12 classes restantes do RVL-CDIP.
\end{tablenotes}

\end{threeparttable}
\end{table*}

%------------------------------------------------
\subsection{Ablation Study: Architecture and Loss Functions}
%------------------------------------------------

The results in Table \ref{tab:results-la-cdip} provide a clear ablation of our framework's components.

\paragraph{Impact of Architecture}
Comparing the standard "Base Model" (which uses mean pooling) with our "Ours (w/ Contrastive Loss)" entry, we expect a significant improvement. This validates our hypothesis that the \textbf{Multi-Query Attention Pooling} mechanism captures richer semantic information from the aligned tokens than simple mean pooling. The \textbf{Residual Head} further ensures that this adaptation does not degrade the pre-trained feature geometry.

\paragraph{Impact of Elastic Losses}
The most significant gains are expected from the use of Angular Margin losses. Standard ArcFace should outperform the Contrastive baseline. However, the \textbf{ElasticArcFace} is expected to achieve the best overall performance. This confirms that introducing stochasticity into the margin calculation helps the model generalize better in few-shot scenarios, preventing overfitting to the limited support set.

\subsection{Proposed Ablation Study: Curriculum Learning Strategy}

Although the full empirical validation of the two-phase curriculum (Section \ref{sec:curriculum}) is ongoing, we propose a structured ablation study to isolate the contribution of each training phase. This study is crucial to verify the hypothesis that a gradual increase in difficulty leads to better convergence and generalization.

We design the following experimental conditions to be evaluated on the LA-CDIP validation set:

\begin{itemize}
    \item \textbf{No Curriculum (Direct EHR):} Training directly with Elastic Margins and Hard Negative Mining from epoch 0. We hypothesize this may lead to early training instability or convergence to suboptimal local minima due to the noise from hard negatives before the manifold is well-formed.
    
    \item \textbf{Static Curriculum (GGA Only):} Training exclusively with the Global Geometric Alignment phase (Fixed Margin, Semi-Hard Mining). This serves as a baseline to measure the specific gain provided by the "Elastic" refinement and hard mining. We expect this to yield stable but less discriminative boundaries.
    
    \item \textbf{Full Two-Phase Curriculum (CaVL-Doc):} The proposed method, transitioning from GGA to EHR. We expect this to achieve the lowest EER by combining early stability with late-stage refinement.
\end{itemize}

This proposed ablation will provide quantitative evidence for the necessity of the phased approach in robust metric learning.

%------------------------------------------------
\subsection{Comparison with State-of-the-Art}
%------------------------------------------------

Our CaVL-Doc framework, using the efficient InternVL3-2B backbone and ElasticArcFace loss, is designed to achieve competitive performance. We aim to demonstrate that a smaller, well-adapted model with a robust metric learning objective can outperform much larger models that rely on brute-force scale.

% --- FIGURA (PLOT DE PARÂMETROS) MANTIDA ---
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{../../results/plots/LA-CDIP_performance_vs_parameters.png}
    \caption{Performance (EER \%) vs. Model Parameters (Log Scale) on the LA-CDIP dataset. Lower EER (y-axis) is better. Our final CaVL-Doc-adapted model achieves the best performance while remaining in the low-parameter (high-efficiency) quadrant.}
    \label{fig:param-plot}
\end{figure*}

\section{Conclusion}\label{sec:conclusion}

In this paper, we proposed CaVL-Doc, a novel framework for efficient Few-Shot Document Classification. We demonstrated that by leveraging the aligned multimodal tokens of a fixed LVLM (InternVL3-2B) and applying a specialized architecture with robust Elastic Margin losses, we can achieve state-of-the-art performance.

\subsection{Summary of Findings}

Our primary contributions are validated by the empirical results on the LA-CDIP dataset:
\begin{itemize}
    \item \textbf{Architectural Efficiency:} We showed that Multi-Query Attention Pooling is superior to standard mean pooling for capturing document semantics.
    
    \item \textbf{Robustness of Elastic Losses:} We demonstrated that "Elastic" angular margin losses (specifically ElasticArcFace) significantly outperform standard contrastive and static margin losses in few-shot scenarios, providing the necessary regularization to prevent overfitting.
    
    \item \textbf{SOTA Performance:} Our 2B parameter model, adapted with CaVL-Doc, outperforms proprietary models like ChatGPT-4o, proving that specialized adaptation is a viable alternative to massive model scaling.
\end{itemize}

\subsection{Future Work}

While our results are promising, this methodology opens several avenues for future research:
\begin{itemize}
    \item \textbf{Adaptive Margin Distributions:} Currently, the parameters of the elastic margin distribution ($\mu, \sigma$) are fixed. Future work could explore learning these parameters dynamically based on class difficulty.
    
    \item \textbf{Hierarchical Attention:} Extending the Multi-Query Attention to a hierarchical structure could allow the model to capture document structure at multiple levels of granularity (e.g., word, line, paragraph).
\end{itemize}

\section*{Declarations}

\begin{itemize}
\item \textbf{Funding} \\
Not applicable.

\item \textbf{Conflict of interest/Competing interests} \\
The authors declare they have no conflicts of interest.

\item \textbf{Ethics approval and consent to participate} \\
Not applicable. This study involves no human participants or animals.

\item \textbf{Consent for publication} \\
Not applicable.

\item \textbf{Data availability} \\
The datasets analyzed during this study, LA-CDIP and RVL-CDIP, are publicly available and were sourced from the authors of \cite{macedo2026vdm}.

\item \textbf{Materials availability} \\
Not applicable.

\item \textbf{Code availability} \\
The source code for the framework and experiments described in this study is available at [GitHub Repository Link, to be added upon publication].

\end{itemize}

%%===================================================%%
%% For presentation purpose, we have included        %%
%% \bigskip command. Please ignore this.             %%
%%===================================================%%
% \bigskip
% \begin{flushleft}%
% Editorial Policies for:

% \bigskip\noindent
% Springer journals and proceedings: \url{https://www.springer.com/gp/editorial-policies}

% \bigskip\noindent
% Nature Portfolio journals: \url{https://www.nature.com/nature-research/editorial-policies}

% \bigskip\noindent
% \textit{Scientific Reports}: \url{https://www.nature.com/srep/journal-policies/editorial-policies}

% \bigskip\noindent
% BMC journals: \url{https://www.biomedcentral.com/getpublished/editorial-policies}
% \end{flushleft}

% \begin{appendices}

% \section{Additional Results}
% \label{appendix:results}

% \begin{table*}[ht]
% \caption{Comparative performance between different visual backbones and Large Language Models. Following the columns: the architecture name; the architecture edition, if exists; cross-validation over the \gls{ZSL} scenario; cross-validation over the \gls{GZSL} scenario; test performance on the \gls{ZSL} scenario; and test performance over the \gls{GZSL} scenario. Every value is a mean EER (\%) value over the \gls{CV} folds.}
% \label{tab:res}
% \centering
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \setlength{\tabcolsep}{6pt} % Adjust column spacing
% \begin{tabular}{lclcccc}
% \toprule
% Architecture                            & Edition & Params & \gls{ZSL}    & \gls{GZSL}    & Test \gls{ZSL} & Test \gls{GZSL} \\
% \midrule
% AlexNet                                 &         &  57M   & 8.92   & 5.45    & 17.33    & 6.31      \\ \midrule
% \multirow{4}{*}{VGG}                    & 11      &  129M  & 7.47   & 5.01    & 14.24    & 3.95      \\
%                                         & 13      &  129M  & 7.03   & 4.79    & 9.30     & 3.95      \\
%                                         & 16      &  134M  & 8.29   & 5.23    & 14.74    & 4.82      \\
%                                         & 19      &  139M  & 7.30   & 4.57    & 17.08    & 3.90      \\ \midrule
% \multirow{5}{*}{ResNet}                 & 18      &  11M   & 5.03   & 1.54    & 4.98     & 1.51      \\
%                                         & 34      &  21M   & 4.32   & 2.10    & 4.13     & 1.53      \\
%                                         & 50      &  23M   & 6.90   & 3.39    & 10.34    & 2.21      \\
%                                         & 101     &  42M   & 8.20   & 2.72    & 11.31    & 1.98      \\
%                                         & 152     &  58M   & 9.44   & 3.38    & 12.70    & 2.39      \\ \midrule
% \multirow{2}{*}{MobileNetV3}            & Small   &  1M    & 7.98   & 5.06    & 12.74    & 5.26      \\
%                                         & Large   &  4M    & 8.16   & 4.27    & 8.45     & 4.43      \\ \midrule
% \multirow{4}{*}{EfficientNet}           & 0       &  4M    & 4.41   & 2.27    & 6.02     & 0.95      \\
%                                         & 1       &  6M    & 3.93   & 3.54    & 8.88     & 2.70      \\
%                                         & 2       &  7M    & 5.73   & 2.61    & 7.29     & 2.14      \\
%                                         & 3       &  10M   & 5.65   & 3.64    & 7.37     & 2.34      \\ \midrule
% \multirow{2}{*}{\gls{ViT}}              & Base    &  87M   & 12.43  & 7.97    & 19.72    & 5.19      \\
%                                         & Large   &  305M  & 13.16  & 7.57    & 19.88    & 5.26      \\ \midrule
% Llama                                   & 3.2     & 11B    & --     & --      & 13.95    & 21.90     \\
% Qwen-VL                                 & 2.5     & 7B     & --     & --      & 6.61     & 4.20      \\
% InternVL                                & 2.5     & 8B     & --     & --      & 8.58     & 10.40     \\
%                                & 3      & 2B     & --     & --      & 38.98    & --        \\
%                                         & 3      & 8B     & --     & --      & 4.04     & --        \\
%                                         & 3     & 14B    & --     & --      & 2.85     & --        \\ \midrule
% \multirow{3}{*}{Siamese InternVL3}      & 1B      & 2B     & --     & --      & 4.83     & --        \\
%                                         & 2B      & 4B     & --     & --      & 2.79     & --        \\
%                                         & 8B      & 16B     & --     & --      & 3.34     & --        \\ \midrule
% MSLA-ZSL                                &         & 4B     & --     & --      & \textbf{1.94} & --    \\ \midrule
% GPT 4o mini                             & 2024-07-18 & *   & --     & --      & 4.70     & 4.07      \\
% GPT 4o                                  & 2024-11-20 & *   & --     & --      & 2.75     & 1.33      \\
% \bottomrule
% \end{tabular}
% \smallskip
% \parbox[t]{\textwidth}{\footnotesize
%     * The parameter count of GPT-4o has not been publicly disclosed.}
% \end{table*}


% \section{Section title of first appendix}\label{secA1}

% An appendix contains supplementary information that is not an essential part of the text itself but which may be helpful in providing a more comprehensive understanding of the research problem or it is information that is too cumbersome to be included in the body of the paper.

%%=============================================%%
%% For submissions to Nature Portfolio Journals %%
%% please use the heading ``Extended Data''.   %%
%%=============================================%%

%%=============================================================%%
%% Sample for another appendix section			       %%
%%=============================================================%%

%% \section{Example of another appendix section}\label{secA2}%
%% Appendices may be used for helpful, supporting or essential material that would otherwise 
%% clutter, break up or be distracting to the text. Appendices can consist of sections, figures, 
%% tables and equations etc.

% \end{appendices}

%%===========================================================================================%%
%% If you are submitting to one of the Nature Portfolio journals, using the eJP submission   %%
%% system, please include the references within the manuscript file itself. You may do this  %%
%% by copying the reference list from your .bbl file, paste it into the main manuscript .tex %%
%% file, and delete the associated \verb+\bibliography+ commands.                            %%
%%===========================================================================================%%
\bibliography{sn-bibliography}
%% if required, the content of .bbl file can be included here once bbl is generated
% \input sn-article.bbl

\end{document}
