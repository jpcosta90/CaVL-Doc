"Authors","Author full names","Author(s) ID","Title","Year","Source title","Volume","Issue","Art. No.","Page start","Page end","Cited by","DOI","Link","Abstract","Document Type","Publication Stage","Open Access","Source","EID"
"de Rodrigo, I.; Sanchez-Cuadrado, A.; Boal, J.; López-López, A.J.","de Rodrigo, Ignacio (58848174200); Sanchez-Cuadrado, Alberto (59342774300); Boal, Jaime (55748682500); López-López, Álvaro J. (55512625200)","58848174200; 59342774300; 55748682500; 55512625200","The MERIT dataset: Modelling and efficiently rendering interpretable transcripts","2026","Pattern Recognition","172","","112502","","","0","10.1016/j.patcog.2025.112502","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017613549&doi=10.1016%2Fj.patcog.2025.112502&partnerID=40&md5=87553d4c03eff472053fda8498aeca82","This paper introduces the MERIT Dataset, a multimodal, fully labeled dataset of school grade reports. Comprising over 400 labels and 33k samples, the MERIT Dataset is a resource for training models in demanding Visually-rich Document Understanding tasks. It contains multimodal features that link patterns in the textual, visual, and layout domains. The MERIT Dataset also includes biases in a controlled way, making it a valuable tool to benchmark biases induced in Language Models. The paper outlines the dataset's generation pipeline and highlights its main features and patterns in its different domains. We benchmark the dataset for token classification, showing that it poses a significant challenge even for SOTA models. © 2025 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-105017613549"
"","","","International Workshops co-located with the 19th International Conference on Document Analysis and Recognition, ICDAR 2025","2026","Lecture Notes in Computer Science","16225 LNCS","","","","","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023387746&partnerID=40&md5=d2dbb361aa4a1abc5cd53119e85dfba4","The proceedings contain 23 papers. The special focus in this conference is on Document Analysis and Recognition. The topics include: Improving Handwritten Text Recognition via 3D Attention and Multi-scale Training; masked Self-supervised Pre-training for Text Recognition Transformers on Large-Scale Datasets; text Prompt to Image Generation for Classification of Similar and Non-similar Scene Images to Improve Text Spotting Performance; Enhancing Document VQA Models via Retrieval-Augmented Generation; a New Multimodal Cross-Domain Network for Classification of Challenging Scene Images; textBite: A Historical Czech Document Dataset for Logical Page Segmentation; few-Part-Shot Font Generation; non-linear Audio-Visual Storytelling from Scanned Comics: A Character-Centric Approach; automatic Text Box Placement for Supporting Typographic Design; visual Document Matching for Zero-Shot Document Classification; evaluating Popular Scene Text Detection and Recognition Methods on Tombstones; deep Learning for Defect Detection in Answer Document Image; ResNet-TPP: A Parallel PHOC-PHOS Framework for Zero-Shot Handwritten Word Recognition in Low-Resource Scripts; Interpret, Prune and Distill Donut: Towards Lightweight VLMs for VQA on Documents; link Prediction Graph Neural Networks for Structure Recognition of Handwritten Mathematical Expressions; rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models; Boosting Handwritten Mathematical Expression Recognition Through Contextual Reasoning with Vision Large Language Models (vLLMs); SCANS: An Efficient Geometric Problem Solver with Content-Aware Attention and Adaptive Fusion; GeoGRPO: Investigating the Stepwise-GRPO Enhancement in RLHF Framework; offline Handwritten Mathematical Formula Recognition Based on Primitive Representation; long Math Reasoning Problem Generation.","Conference review","Final","","Scopus","2-s2.0-105023387746"
"de Almeida Bandeira Macedo, L.; Costa, J.P.; Felix de Almeida, J.P.; Garcia Freitas, P.; Li, L.","de Almeida Bandeira Macedo, Lucas (60112467400); Costa, João Paulo Vieira (57224310790); Felix de Almeida, João Pedro (60112510000); Garcia Freitas, Pedro Garcia (55431728000); Li, Weigang (52163887600)","60112467400; 57224310790; 60112510000; 55431728000; 52163887600","Visual Document Matching for Zero-Shot Document Classification","2026","Lecture Notes in Computer Science","16225 LNCS","","","192","208","0","10.1007/978-3-032-09368-4_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023375581&doi=10.1007%2F978-3-032-09368-4_12&partnerID=40&md5=d7f98d23712611121010f6435541d993","Accurate document identification is crucial for ensuring compliance and maintaining consistency across various applications. Consequently, document classification has been extensively studied. However, the dynamic nature of documents often renders simple classification insufficient, necessitating frequent model retraining. To mitigate this constant maintenance, comparing incoming documents against known references is a more suitable approach. To address the zero-shot classification problem, we introduce Visual Document Matching (VDM). VDM focuses on verifying whether two documents share an identical visual layout structure, a particularly effective approach in scenarios where training classes do not align with those encountered during inference. Despite its significant potential, the Zero-Shot Learning (ZSL) approach remains largely underexplored in document layout understanding. To support our study, we introduce Layout-Aware Complex Document Information Processing (LA-CDIP), a dataset comprising 4,993 documents across 144 classes. We reorganized this dataset from the RVL-CDIP database to emphasize visual structure over semantic information. Our approach is benchmarked using a siamese network within a contrastive learning framework across multiple backbone architectures, including ResNet, EfficientNet, and Vision Transformer (ViT). In zero-shot scenarios, our method achieves an Equal Error Rate (EER) below 5% in 1-vs-1 verification with cross-validation. Furthermore, our VDM approach outperforms lighter Large Language Models (LLMs) and rivals GPT-4o. These findings highlight the superiority of specialized VDM techniques over general-purpose multimodal models, demonstrating high accuracy with significantly fewer parameters, making our approach more practical for real-world applications. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2026.","Conference paper","Final","","Scopus","2-s2.0-105023375581"
"Pal, A.; Biswas, S.; Das, A.; Lodh, A.; Banerjee, P.; Chattopadhyay, S.; Mondal, A.; Karatzas, D.; Llados, J.; Jawahar, C.V.","Pal, Aniket (57467826800); Biswas, Sanket (57226196113); Das, Alloy (57560089800); Lodh, Ayush (59802348000); Banerjee, Priyanka (59512809300); Chattopadhyay, Soumitri (57289784800); Mondal, Ajoy (54899188400); Karatzas, Dimosthenis (16030882900); Llados, Josep (6603062543); Jawahar, C. V. (6701503911)","57467826800; 57226196113; 57560089800; 59802348000; 59512809300; 57289784800; 54899188400; 16030882900; 6603062543; 6701503911","ICDAR 2025 Handwritten Notes Understanding Challenge","2026","Lecture Notes in Computer Science","16027 LNCS","","","553","567","0","10.1007/978-3-032-04630-7_32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016906018&doi=10.1007%2F978-3-032-04630-7_32&partnerID=40&md5=311e01600685ffa995fcaea34836bab4","This report presents the results of the inaugural ICDAR 2025 Handwritten Notes Understanding Competition, centered on evidence-based question answering over complex scientific handwritten notes. The final competition test set included over 2,000 real academic note images and 1,000 curated questions across diverse STEM domains, characterized by varied layouts, diagrams, and dense equations. The overall challenge was structured as a single task with a multi-phase evaluation protocol designed to assess not only answer correctness but also grounded reasoning ability, requiring models to localize the exact visual evidence supporting their answers. Hosted on the Robust Reading Challenge (RRC) Portal, the competition ensured fair, private-set benchmarking, with 6 valid submissions out of 16 registered teams. The results underscore the obstacles that current state-of-the-art (SOTA) vision-language models (VLMs) face in multimodal reasoning over handwritten content. This first edition marks a promising step toward interpretable, layout-aware document understanding and sets the foundation for future iterations. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2026.","Conference paper","Final","","Scopus","2-s2.0-105016906018"
"Vaiani, L.; Ding, Y.; Cagliero, L.; Lee, J.; Garza, P.; Poon, J.; Han, S.C.","Vaiani, Lorenzo (57698759400); Ding, Yihao (57748659500); Cagliero, Luca (35190145000); Lee, Jean (57224911298); Garza, Paolo (36909944300); Poon, Josiah (7005903715); Han, Soyeon Caren (55361191400)","57698759400; 57748659500; 35190145000; 57224911298; 36909944300; 7005903715; 55361191400","KIEPrompter: Leveraging Lightweight Models' Predictions for Cost-Effective Key Information Extraction using Vision LLMs","2025","","","","","2925","2934","0","10.1145/3746252.3761416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023191912&doi=10.1145%2F3746252.3761416&partnerID=40&md5=4ae170ff9b9a5c8b50ecf9937bafee6f","Key information extraction (KIE) from visually rich documents, such as receipts and forms, involves a deep understanding of textual, visual, and layout feature information. Transformers fine-tuned for KIE achieve state-of-the-art performance but lack generality and portability across different domains. In contrast, vision large language models (VLLMs) offer higher flexibility and zero-shot capability but fall short with domain-specific layout relations unless performing a resource-demanding supervised fine-tuning. To reach the best compromise solution between lightweight models and VLLMs, we propose KIEPrompter, a cost-effective LLM-based KIE approach that leverages the predictions of lightweight models as external knowledge injected into VLLM prompts. By incorporating these auxiliary predictions, VLLMs are guided to attend relevant multimodal content without ad hoc training. The accuracy results achieved by KIEPrompter in three benchmark document collections are superior to those of VLLMs in both zero-shot and layout-sensitive scenarios. We compare various strategies for incorporating lightweight model predictions, ranging from coarse-grained predictions without explicit confidence scores to fine-grained per-element network logits. We also demonstrate that our approach is robust to the absence of specific classes in trained lightweight models, as the VLLMs' pre-training compensates for the limited generality of lightweight models. © 2025 Copyright held by the owner/author(s).","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105023191912"
"Vafaie, M.; Hertling, S.; Banse-Strobel, I.; Dubout, K.; Sack, H.","Vafaie, Mahsa (57312115600); Hertling, Sven (55583440200); Banse-Strobel, Inger (60215079600); Dubout, Kevin (60214384400); Sack, Harald (7102918498)","57312115600; 55583440200; 60215079600; 60214384400; 7102918498","End-to-end Information Extraction from Archival Records with Multimodal Large Language Models","2025","","","","","6075","6083","0","10.1145/3746252.3761503","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105023170599&doi=10.1145%2F3746252.3761503&partnerID=40&md5=b620d0f820abb327462b236d9447de1e","Semi-structured Document Understanding presents a challenging research task due to the significant variations in layout, style, font, and content of documents. This complexity is further amplified when dealing with born-analogue historical documents, such as digitised archival records, which contain degraded print, handwritten annotations, stamps, marginalia and inconsistent formatting resulting from historical production and digitisation processes. Traditional approaches for extracting information from semi-structured documents rely on manual labour, making them costly and inefficient. This is partly due to the fact that within document collections, there are various layout types, each requiring customised optimisation to account for structural differences, which substantially increases the effort needed to achieve consistent quality. The emergence of Multimodal Large Language Models (MLLMs) has significantly advanced Document Understanding by enabling flexible, prompt-based understanding of document images, needless of OCR outputs or layout encodings. Moreover, the encoder-decoder architectures have overcome the limitations of encoder-only models, such as reliance on annotated datasets and fixed input lengths. However, there still remains a gap in effectively applying these models in real-world scenarios. To address this gap, we first introduce BZKOpen, a new annotated dataset designed for key information extraction from historical German index cards. Furthermore, we systematically assess the capabilities of several state-of-the-art MLLMs-including the open-source InternVL2.0 and InternVL2.5 series, and the commercial GPT-4o-mini-on the task of extracting key information from these archival documents. Both zero-shot and few-shot prompting strategies are evaluated across different model configurations to identify the optimal conditions for performance. Interestingly, our results reveal that increasing model size does not necessarily lead to better performance on this dataset. Among all models tested, the open-source InternVL2.5-38B consistently achieves the most robust results, outperforming both larger InternVL models and the proprietary alternative. We further provide practical insights into prompt engineering and inference settings, offering guidance for applying MLLMs to real-world key information extraction tasks. Additionally, we highlight the need for more ground truth datasets that include a wider range of historical documents with varying quality and in multiple languages, in order to fully explore the potentials and limitations of MLLMs for key information extraction from historical records. © 2025 Copyright held by the owner/author(s).","Conference paper","Final","","Scopus","2-s2.0-105023170599"
"Chen, L.; Xiao, Z.; Wang, J.; Huang, Z.; Zeng, Y.; Xu, J.","Chen, Longfeng (60114309800); Xiao, Zheng (60227571200); Wang, Juyuan (59536658100); Huang, Zeyu (60113965100); Zeng, Yawen (57224734965); Xu, Jin (57189710200)","60114309800; 60227571200; 59536658100; 60113965100; 57224734965; 57189710200","HEAR: A Holistic Extraction and Agentic Reasoning Framework for Document Understanding","2025","","","","","14376","14382","0","10.1145/3746027.3761999","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105024062547&doi=10.1145%2F3746027.3761999&partnerID=40&md5=468fcc45bfa094dee6091a29f9fce1e1","The automated comprehension of complex, multi-modal documents is fundamentally hampered by a disconnect between information extraction and reasoning. Existing systems suffer from inherent limitations. Monolithic models embed reasoning as a black box process, sacrificing transparency and depth. Meanwhile, current agent-based frameworks follow a passive, non-interactive paradigm; they handle static, global inputs rather than information derived from active exploration, which fundamentally restricts their ability to achieve structural understanding and complex reasoning. To bridge this critical gap, we introduce HEAR, a framework for Holistic Extraction and Agentic Reasoning. This innovative framework establishes a synergistic, closed-loop between a deep Vision-Language Model (VLM) driven holistic parsing engine and a collaborative multi-agent reasoning system. Our HEAR initially transforms unstructured documents into a semantically-rich, structured representation, preserving complex layouts and reconstituting multi-page tables. Subsequently, a multi-agent system performs cross-modal analysis, governed by a crucial verification protocol that forces agents to validate findings across textual and visual modalities. A conflict driven re-evaluation mechanism enables the system to dynamically re-engage the document to resolve ambiguities, thereby unifying the perception-cognition cycle. HEAR achieved first place in the ACM MM 2025 Grand Challenge on Large Vision-Language Model Learning and Applications. © 2025 ACM.","Conference paper","Final","","Scopus","2-s2.0-105024062547"
"Yang, Z.; Hua, W.; Song, S.; Yao, C.; Zhu, Y.; Cheng, W.; Bai, X.","Yang, Zhibo (57205359678); Hua, Wei (58151467400); Song, Sibo (57701931900); Yao, Cong (55336187500); Zhu, Yingying (58736581000); Cheng, Wenqing (59672387200); Bai, Xiang (15130916600)","57205359678; 58151467400; 57701931900; 55336187500; 58736581000; 59672387200; 15130916600","Generative compositor for few-shot visual information extraction","2025","Pattern Recognition","165","","111624","","","0","10.1016/j.patcog.2025.111624","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105001947462&doi=10.1016%2Fj.patcog.2025.111624&partnerID=40&md5=034ddd85313da82104a1736fe178f8ee","Visual Information Extraction (VIE), aiming at extracting structured information from visually rich document images, plays a pivotal role in document processing. Considering various layouts, semantic scopes, and languages, VIE encompasses an extensive range of types, potentially numbering in the thousands. However, many of these types suffer from a lack of training data, which poses significant challenges. In this paper, we propose a novel generative model, named Generative Compositor, to address the challenge of few-shot VIE. The Generative Compositor is a hybrid pointer-generator network that emulates the operations of a compositor by retrieving words from the source text and assembling them based on the provided prompts. Furthermore, three pre-training strategies are employed to enhance the model's perception of spatial context information. Besides, a prompt-aware resampler is specially designed to enable efficient matching by leveraging the entity-semantic prior contained in prompts. The introduction of the prompt-based retrieval mechanism and the pre-training strategies enable the model to acquire more effective spatial and semantic clues with limited training samples. Experiments demonstrate that the proposed method achieves highly competitive results in the full-sample training, while notably outperforms the baseline in the 1-shot, 5-shot, and 10-shot settings. © 2025","Article","Final","","Scopus","2-s2.0-105001947462"
"Zhao, R.; Ou Yang, J.J.; Gao, C.; Qin, X.; Zeng, G.; Hu, X.; Zhang, P.","Zhao, Runbo (58175204900); Ou Yang, Junjie (59475812000); Gao, Chen (59162250100); Qin, Xugong (57215089190); Zeng, Gangyan (57215816277); Hu, Xiaoxu (59309764900); Zhang, Peng (59876105100)","58175204900; 59475812000; 59162250100; 57215089190; 57215816277; 59309764900; 59876105100","Perception-Enhanced Generative Transformer for Key Information Extraction from Documents","2025","Lecture Notes in Computer Science","15331 LNCS","","","91","106","0","10.1007/978-3-031-78119-3_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212271826&doi=10.1007%2F978-3-031-78119-3_7&partnerID=40&md5=d1318a47b7d45a7d7b765f4e9422d8af","Key information extraction (KIE) from scanned documents has attracted significant attention due to practical real-world applications. Despite impressive results achieved by incorporating multimodal information within the generative framework, existing methods fail to understand complex layouts and fuzzy semantics in document images. To settle these issues, we propose a perception-enhanced generative transformer (PEGT), which improves the model through fine-grained multimodal modeling and pre-training tasks tailored for the generative framework. Firstly, we introduce a pre-trained vision-language model to provide transferable knowledge for visual text perceptron. Then two auxiliary pre-training tasks including absolute position prediction (APP) and semantic relationship reasoning (SRR) are designed for the generative framework. APP learns to predict which grids the texts fall into, improving the model on utilization of the position information. SRR exploits prior information of semantic relationships, injecting the ability for better semantic discrimination into PEGT. Finally, well-designed prompts are leveraged to unleash the potential of PEGT for extracting key information from documents. Extensive experiments on several public datasets show that PEGT effectively generalizes over different types of documents. Especially, PEGT achieves state-of-the-art results in terms of F-measure, i.e., 97.47%, 98.04%, and 84.32% on the SROIE, CORD, and FUNSD datasets, demonstrating the superiority of the proposed method. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.","Conference paper","Final","","Scopus","2-s2.0-85212271826"
"Scius-Bertrand, A.; Jungo, M.; Vogtlin, L.; Spat, J.-M.; Fischer, A.","Scius-Bertrand, Anna (56039895600); Jungo, Michael (57219809919); Vogtlin, Lars (57210751102); Spat, Jean Marc (59476360800); Fischer, Andreas (57192656275)","56039895600; 57219809919; 57210751102; 59476360800; 57192656275","Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image Classification Using Large Language Models","2025","Lecture Notes in Computer Science","15319 LNCS","","","152","166","5","10.1007/978-3-031-78495-8_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85212257751&doi=10.1007%2F978-3-031-78495-8_10&partnerID=40&md5=795b9e8ba095fe441d263d2b4726a5f8","Classifying scanned documents is a challenging problem that involves image, layout, and text analysis for document understanding. Nevertheless, for certain benchmark datasets, notably RVL-CDIP, the state of the art is closing in to near-perfect performance when considering hundreds of thousands of training samples. With the advent of large language models (LLMs), which are excellent few-shot learners, the question arises to what extent the document classification problem can be addressed with only a few training samples, or even none at all. In this paper, we investigate this question in the context of zero-shot prompting and few-shot model fine-tuning, with the aim of reducing the need for human-annotated training samples as much as possible. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2025.","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85212257751"
"Long, R.; Xing, H.; Yang, Z.; Zheng, Q.; Yu, Z.; Huang, F.; Yao, C.","Long, Rujiao (57208776553); Xing, Hangdi (57837223800); Yang, Zhibo (57205359678); Zheng, Qi (57205367369); Yu, Zhi (56589267500); Huang, Fei (57210150087); Yao, Cong (55336187500)","57208776553; 57837223800; 57205359678; 57205367369; 56589267500; 57210150087; 55336187500","LORE++: Logical location regression network for table structure recognition with pre-training","2025","Pattern Recognition","157","","110816","","","10","10.1016/j.patcog.2024.110816","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85201309450&doi=10.1016%2Fj.patcog.2024.110816&partnerID=40&md5=12c10be18ad8bc2ddf85a3ff41562e49","Table structure recognition (TSR) aims at extracting tables in images into machine-understandable formats. Current approaches address this issue by either predicting the adjacency of detected cells or direct generation of structural sequences. Nonetheless, these approaches either count on additional heuristic rules for post-processing, or involve the generation of extremely long-range sequences that lead to computational intricacy. In this paper, We redefine TSR as a LOgical location REgression paradigm, which effectively captures inherent logical dependencies and constraints among table cells. Correspondingly, we propose LORE, a novel approach for TSR. LORE simultaneously predicts accurate geometric coordinates of table cells and the logical structures of the entire table. Our proposed LORE is conceptually simpler, easier to train, and more accurate than other TSR paradigms. Moreover, to enhance the model's spatial and logical representation capabilities, we propose two pre-training tasks, resulting in an upgraded version named LORE++. The incorporation of pre-training is proven to enjoy significant advantages, leading to a substantial enhancement in terms of accuracy, generalization, and few-shot capability compared to its predecessor. Experiments on standard benchmarks demonstrate the superiority of LORE++, which highlights the potential and promising prospect of the logical location regression paradigm for TSR. © 2024 Elsevier Ltd","Article","Final","","Scopus","2-s2.0-85201309450"
"L.; Costa, J.P.; J.P.F.; Garcia Freitas, P.; Li, L.","; Costa, Joaõ Paulo Carvalho Lustosa Da (57188933911); Garcia Freitas, Pedro Garcia (55431728000); Li, Weigang (52163887600)","60244822700; 57188933911; 60244615200; 55431728000; 52163887600","Towards Zero-Shot Document Image Classification","2025","Brazilian Symposium of Computer Graphic and Image Processing","","","","","","0","10.1109/SIBGRAPI67909.2025.11223349","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105025139683&doi=10.1109%2FSIBGRAPI67909.2025.11223349&partnerID=40&md5=1739bb190490afe6f5c948d5d575a275","Classification is a fundamental tool to automate the process of categorizing documents in many real-world applications, such as information management, financial document processing, healthcare records management, news categorization, fraud detection, regulatory compliance, and many others. Because of this broad spectrum of applications, document classification is of paramount importance for various companies. However, documents often change in terms of format and their visual patterns, which may impair a simple classification model. Moreover, model continuance and retraining often demands important efforts, consuming computational resources and demanding new data. Therefore, techniques capable of classifying documents by simply observing new data, without necessarily requiring retraining the classifier, are of immense importance for a wide variety of applications. In this context, Zero-Shot Learning (ZSL) is especially suitable for document classification because it handles diverse and ever-changing document content. In this work, we tackle the gap involving Zero-Shot Document Image Classification (ZS-DIC), where we classify documents that have not been seen by the model during training. To achieve this, we built Layout-Aware Complex Document Information Processing (LA-CDIP), a dataset tailored for this problem. LAC-DIP prioritizes structural consistency, allowing models to classify documents correctly under a ZSL scenario. To benchmark this dataset, we developed a series of Siamese Neural Networks (NNs) based on a variety of computer vision neural architectures, such as ResNet, EfficientNet, ViT and others. As a result, the proposed ZSL-based method achieves Equal Error Rates (EERs) under 5%. The code of the proposed method is available at https://github.com/ABMHub/doc-zsl. © 2025 IEEE.","Conference paper","Final","","Scopus","2-s2.0-105025139683"
"Wang, K.; Toibazar, D.; Alfulayt, A.; Albadawi, A.S.; Alkahtani, R.A.; Ibrahim, A.A.; Alhomoud, H.A.; Mohamed, S.; Moreno, P.J.","Wang, Kesen (60089147800); Toibazar, Daulet (59897473600); Alfulayt, Abdulrahman (60089470600); Albadawi, Abdulaziz S. (60089039000); Alkahtani, Ranya (59717495300); Ibrahim, Asma A. (59716747700); Alhomoud, Haneen (59332897900); Mohamed, Sherif (60089470700); Moreno, Pedro J. (60088930000)","60089147800; 59897473600; 60089470600; 60089039000; 59717495300; 59716747700; 59332897900; 60089470700; 60088930000","Multi-Agent Interactive Question Generation Framework for Long Document Understanding","2025","IEEE International Workshop on Machine Learning for Signal Processing, MLSP","","","","","","0","10.1109/MLSP62443.2025.11204232","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105022086088&doi=10.1109%2FMLSP62443.2025.11204232&partnerID=40&md5=459b23c9510f613ffb77185ab2bdffd3","Document Understanding (DU) in long-contextual scenarios with complex layouts remains a significant challenge in vision-language research. Although Large Vision-Language Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context settings. A key limitation is the scarcity of fine-grained training data, particularly for low-resource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human annotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive framework to generate long-context questions efficiently. Our approach efficiently generates high-quality single- and multi-page questions for extensive English and Arabic documents, covering hundreds of pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context understanding ability. Experimental results in this work have shown that our generated English and Arabic questions (AraEngLongBench) are quite challenging to major open- and close-source LVLMs. The code and data proposed in this work can be found in https://github.com/ wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and Answer (QA) pairs and structured system prompts can be found in the Appendix. © 2025 IEEE.","Conference paper","Final","","Scopus","2-s2.0-105022086088"
"Deng, C.; Yuan, J.; Bu, P.; Wang, P.; Li, Z.-Z.; Xu, J.; Li, X.-H.; Gao, Y.; Song, J.; Zheng, B.; Liu, C.-L.","Deng, Chao (59510504300); Yuan, Jiale (59156177200); Bu, Pi (59361765100); Wang, Peijie (59510579200); Li, Zhongzhi (58753030300); Xu, Jian (57861126300); Li, Xiaohui (57203016574); Gao, Yuan (59156568500); Song, Jun (58135142300); Zheng, Bo (57214453009); Liu, Chenglin (36064176500)","59510504300; 59156177200; 59361765100; 59510579200; 58753030300; 57861126300; 57203016574; 59156568500; 58135142300; 57214453009; 36064176500","LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating","2025","Proceedings of the Annual Meeting of the Association for Computational Linguistics","1","","","1135","1159","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105021057050&partnerID=40&md5=5b0de3a3cd9eb47917840f04be42e0bd","Large vision language models (LVLMs) have improved the document understanding capabilities remarkably, enabling the handling of complex document elements, longer contexts, and a wider range of tasks. However, existing document understanding benchmarks have been limited to handling only a small number of pages and fail to provide a comprehensive analysis of layout elements locating. In this paper, we first define three primary task categories: Long Document Understanding, numerical Reasoning, and cross-element Locating, and then propose a comprehensive benchmark-LongDocURL-integrating above three primary tasks and comprising 20 sub-tasks categorized based on different primary tasks and answer evidences. Furthermore, we develop a semi-automated construction pipeline and collect 2,325 high-quality question-answering pairs, covering more than 33,000 pages of documents, significantly outperforming existing benchmarks. Subsequently, we conduct comprehensive evaluation experiments on both open-source and closed-source models across 26 different configurations, revealing critical performance gaps in this field. The code and data: https://github.com/dengc2023/LongDocURL. © 2025 Association for Computational Linguistics.","Conference paper","Final","","Scopus","2-s2.0-105021057050"
"Morin, L.; Weber, V.; Nassar, A.; Meijer, G.I.; van Gool, L.; Li, Y.; Staar, P.","Morin, Lucas (58574428300); Weber, Valéry T. (7005235316); Nassar, Ahmed S. (57222289265); Meijer, Gerhard Ingmar (36000026700); van Gool, Luc J. (22735702300); Li, Yawei (57189730893); Staar, Peter W.J. (45561795700)","58574428300; 7005235316; 57222289265; 36000026700; 22735702300; 57189730893; 45561795700","MarkushGrapher: Joint Visual and Textual Recognition of Markush Structures","2025","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","14505","14515","1","10.1109/CVPR52734.2025.01352","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017087536&doi=10.1109%2FCVPR52734.2025.01352&partnerID=40&md5=5e0e048a59272429cddba585c44d5891","The automated analysis of chemical literature holds promise to accelerate discovery in fields such as material science and drug development. In particular, search capabilities for chemical structures and Markush structures (chemical structure templates) within patent documents are valuable, e.g., for prior-art search. Advancements have been made in the automatic extraction of chemical structures from text and images, yet the Markush structures remain largely unexplored due to their complex multi-modal nature. In this work, we present MarkushGrapher, a multimodal approach for recognizing Markush structures in documents. Our method jointly encodes text, image, and layout information through a Vision-Text-Layout encoder and an Optical Chemical Structure Recognition vision encoder. These representations are merged and used to autoregressively generate a sequential graph representation of the Markush structure along with a table defining its variable groups. To overcome the lack of real-world training data, we propose a synthetic data generation pipeline that produces a wide range of realistic Markush structures. Additionally, we present M2S, the first annotated benchmark of real-world Markush structures, to advance research on this challenging task. Extensive experiments demonstrate that our approach outperforms state-of-the-art chemistry-specific and general-purpose vision-language models in most evaluation settings. Code, models, and datasets are available1 © 2025 IEEE.","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-105017087536"
"Duan, Y.; Chen, Z.; Hu, Y.; Wang, W.; Ye, S.; Shi, B.; Lu, L.; Hou, Q.; Lu, T.; Li, H.; Dai, J.; Wang, W.","Duan, Yuchen (57719405500); Chen, Zhe (57771212400); Hu, Yusong (57937962300); Wang, Weiyun (57983006500); Ye, Shenglong (59048037300); Shi, Botian (57211759002); Lu, Lewei (57219510289); Hou, Qibin (57191428893); Lu, Tong (27169293700); Li, Hongsheng (57141098300); Dai, Jifeng (57141385600); Wang, Wenhai (57192992309)","57719405500; 57771212400; 57937962300; 57983006500; 59048037300; 57211759002; 57219510289; 57191428893; 27169293700; 57141098300; 57141385600; 57192992309","Docopilot: Improving Multimodal Models for Document-Level Understanding","2025","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","4026","4037","2","10.1109/CVPR52734.2025.00381","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017057324&doi=10.1109%2FCVPR52734.2025.00381&partnerID=40&md5=4e5f28fe07f34ed446aa9949eff3b2e4","Despite significant progress in multimodal large language models (MLLMs), their performance on complex, multi-page document comprehension remains inadequate, largely due to the lack of high-quality, document-level datasets. While current retrieval-augmented generation (RAG) methods offer partial solutions, they suffer from issues, such as fragmented retrieval contexts, multi-stage error accumulation, and extra time costs of retrieval. In this work, we present a high-quality document-level dataset, Doc-750K, designed to support in-depth understanding of multimodal documents. This dataset includes diverse document structures, extensive cross-page dependencies, and real question-answer pairs derived from the original documents. Building on the dataset, we develop a native multimodal model - Docopilot, which can accurately handle document-level dependencies without relying on RAG. Experiments demonstrate that Docopilot achieves superior coherence, accuracy, and efficiency in document understanding tasks and multi-turn interactions, setting a new baseline for document-level multimodal understanding. Data, code, and models are released at https://github.com/OpenGVLab/Docopilot. © 2025 IEEE.","Conference paper","Final","","Scopus","2-s2.0-105017057324"
"Guo, H.; Qin, X.; Ou Yang, J.J.; Zhang, P.; Zeng, G.; Li, Y.; Lin, H.","Guo, Hao (60114124500); Qin, Xugong (57215089190); Ou Yang, Junjie (59475812000); Zhang, Peng (59876105100); Zeng, Gangyan (57215816277); Li, Yubo (58538630700); Lin, Hailun (56482451400)","60114124500; 57215089190; 59475812000; 59876105100; 57215816277; 58538630700; 56482451400","Towards Natural Language-Based Document Image Retrieval: New Dataset and Benchmark","2025","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","29722","29732","0","10.1109/CVPR52734.2025.02767","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017030971&doi=10.1109%2FCVPR52734.2025.02767&partnerID=40&md5=7895a69b4cc4d124d377489b48487aff","Document image retrieval (DIR) aims to retrieve document images from a gallery according to a given query. Existing DIR methods are primarily based on image queries that retrieve documents within the same coarse semantic category, e.g., newspapers or receipts. However, these methods struggle to effectively retrieve document images in real-world scenarios where textual queries with fine-grained semantics are usually provided. To bridge this gap, we introduce a new Natural Language-based Document Image Retrieval (NL-DIR) benchmark with corresponding evaluation metrics. In this work, natural language descriptions serve as semantically rich queries for the DIR task. The NL-DIR dataset contains 41K authentic document images, each paired with five high-quality, fine-grained semantic queries generated and evaluated through large language models in conjunction with manual verification. We perform zero-shot and fine-tuning evaluations of existing mainstream contrastive vision-language models and OCR-free visual document understanding (VDU) models. A two-stage retrieval method is further investigated for performance improvement while achieving both time and space efficiency. We hope the proposed NL-DIR benchmark can bring new opportunities and facilitate research for the VDU community. Datasets and codes will be publicly available at huggingface.co/datasets/nianbing/NL-DIR. © 2025 IEEE.","Conference paper","Final","","Scopus","2-s2.0-105017030971"
"Nacson, M.S.; Aberdam, A.; Ganz, R.; Avraham, E.B.; Golts, A.; Kittenplon, Y.; Mazor, S.; Litman, R.","Nacson, Mor Shpigel (57204920489); Aberdam, Aviad (57200090557); Ganz, Roy (57223730652); Avraham, Elad Ben (58858910700); Golts, Alona (57189442329); Kittenplon, Yair (57221116403); Mazor, Shai (57194051337); Litman, Ron (57219703659)","57204920489; 57200090557; 57223730652; 58858910700; 57189442329; 57221116403; 57194051337; 57219703659","DocVLM: Make Your VLM an Efficient Reader","2025","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","29005","29015","2","10.1109/CVPR52734.2025.02701","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105017027331&doi=10.1109%2FCVPR52734.2025.02701&partnerID=40&md5=be357c5c043bb597e25414a3c2dc312f","Vision-Language Models (VLMs) excel in diverse visual tasks but face challenges in document understanding, which requires fine-grained text processing. While typical visual tasks perform well with low-resolution inputs, reading-intensive applications demand high-resolution, resulting in significant computational overhead. Using OCR-extracted text in VLM prompts partially addresses this issue but underperforms compared to full-resolution counterpart, as it lacks the complete visual context needed for optimal performance. We introduce DocVLM, a method that integrates an OCR-based modality into VLMs to enhance document processing while preserving original weights. Our approach employs an OCR encoder to capture textual content and layout, compressing these into a compact set of learned queries incorporated into the VLM. Comprehensive evaluations across leading VLMs show that DocVLM significantly reduces reliance on high-resolution images for document understanding. In limited-token regimes (448×448), DocVLM with 64 learned queries improves DocVQA results from 56.0% to 86.6% when integrated with InternVL2 and from 84.4% to 91.2% with Qwen2-VL. In LLaVA-OneVision, DocVLM achieves improved results while using 80% less image tokens. The reduced token usage allows processing multiple pages effectively, showing impressive zero-shot results on DUDE and state-of-the-art performance on MP-DocVQA, highlighting DocVLM's potential for applications requiring high-performance and efficiency. © 2025 IEEE.","Conference paper","Final","","Scopus","2-s2.0-105017027331"
"Son, H.H.; Duc-Dung, N.; Phuong, N.T.; Giang Son, T.","Son, Hoang Huu (57210846293); Duc-Dung, Nguyen (57190166348); Phuong, Nghiem Thi (60102818700); Giang Son, Tran (60102818800)","57210846293; 57190166348; 60102818700; 60102818800","Improving Table Structure Recognition Based on Content-Based Post-Processing","2025","","","","","","","0","10.1109/MAPR67746.2025.11133962","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105016160257&doi=10.1109%2FMAPR67746.2025.11133962&partnerID=40&md5=d96c6f1a9d299768cc1e9b7bdef4a6da","Table Structure Recognition (TSR) is a critical task in document understanding, particularly in financial and administrative domains where table layouts follow strict formatting rules. While state-of-the-art models such as CascadeTSRNet offer high performance in recognizing table structures, they often overlook content-level semantics, leading to errors in logically structured tables. In this paper, we propose a lightweight and effective post-processing method based on content analysis and rule-based correction, applied after table structure recognition. By combining OCR outputs with heuristic rules derived from financial document standards, our method identifies and rectifies formatting inconsistencies to enhance logical structure. Experimental results on the FinTabNet.C dataset demonstrate that this approach improves the overall TEDS score and increases the number of perfectly recognized tables (TEDS = 1), while requiring minimal computational overhead. Our method also shows promising performance when integrated with both traditional OCR engines and large vision-language models (e.g., Qwen2.5-VL-3B-Instruct). © 2025 IEEE.","Conference paper","Final","","Scopus","2-s2.0-105016160257"
"Tanasa, A.-M.; Oprea, S.-V.","Tanasa, Andreea Maria (60006700800); Oprea, Simona Vasilica (30067888400)","60006700800; 30067888400","Rethinking Chart Understanding Using Multimodal Large Language Models","2025","Computers, Materials and Continua","84","2","","2905","2933","0","10.32604/cmc.2025.065421","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105011168144&doi=10.32604%2Fcmc.2025.065421&partnerID=40&md5=ec12808dc8f7652045dd4e85a986ef44","Extracting data from visually rich documents and charts using traditional methods that rely on OCR-based parsing poses multiple challenges, including layout complexity in unstructured formats, limitations in recognizing visual elements, and the correlation between different parts of the documents, as well as domain-specific semantics. Simply extracting text is not sufficient; advanced reasoning capabilities are proving to be essential to analyze content and answer questions accurately. This paper aims to evaluate the ability of the Large Language Models (LLMs) to correctly answer questions about various types of charts, comparing their performance when using images as input versus directly parsing PDF files. To retrieve the images from the PDF, ColPali, a model leveraging state-of-the-art visual language models, is used to identify the relevant page containing the appropriate chart for each question. Google’s Gemini multimodal models were used to answer a set of questions through two approaches: 1) processing images derived from PDF documents and 2) directly utilizing the content of the same PDFs. Our findings underscore the limitations of traditional OCR-based approaches in visual document understanding (VrDU) and demonstrate the advantages of multimodal methods in both data extraction and reasoning tasks. Through structured benchmarking of chart question answering (CQA) across input formats, our work contributes to the advancement of chart understanding (CU) and the broader field of multimodal document analysis. Using two diverse and information-rich sources: the World Health Statistics 2024 report by the World Health Organisation and the Global Banking Annual Review 2024 by McKinsey & Company, we examine the performance of multimodal LLMs across different input modalities, comparing their effectiveness in processing charts as images versus parsing directly from PDF content. These documents were selected due to their multimodal nature, combining dense textual analysis with varied visual representations, thus presenting realistic challenges for vision-language models. This comparison is aimed at assessing how advanced models perform with different input formats and to determine if an image-based approach enhances chart comprehension in terms of accurate data extraction and reasoning capabilities. © © 2025 The Authors.","Article","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-105011168144"
"Zhang, P.; Liu, Z.; Xiao, S.; Shao, N.; Ye, Q.; Dou, Z.","Zhang, Peitian (57313719200); Liu, Zheng (57211759701); Xiao, Shitao (57223161443); Shao, Ninglu (57938995100); Ye, Qiwei (59437989000); Dou, Zhicheng (24722777200)","57313719200; 57211759701; 57223161443; 57938995100; 59437989000; 24722777200","LONG CONTEXT COMPRESSION WITH ACTIVATION BEACON","2025","","","","","1768","1784","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010225903&partnerID=40&md5=2e7a70e825646c595a3cf98bc5eaf9f0","Long context compression is a critical research problem due to its significance in reducing the high computational and memory costs associated with LLMs. In this paper, we propose Activation Beacon, a plug-in module for transformer-based LLMs that targets effective, efficient, and flexible compression of long contexts. To achieve this, our method introduces the following technical designs. 1) We directly compress the activations (i.e. keys and values at every layer), rather than leveraging soft prompts to relay information (which constitute a major bottleneck to encapsulate the complex information within long contexts). 2) We tailor the compression workflow, where each fine-grained input unit is progressively compressed, enabling high-quality compression and efficient computation during both training and inference. 3) We train the model through compression-based auto-regression, making full use of plain texts and instructional data to optimize the model's compression performance. 4) During training, we randomly sample a compression ratio at each step, teaching the model to support a wide range of compression configurations. Extensive evaluations are conducted on various long-context tasks whose lengths (e.g., 128K) may far exceed the maximum training length (20K), such as document understanding, few-shot learning, and Needle-in-a-Haystack. Whilst existing methods struggle to handle these challenging tasks, Activation Beacon maintains a comparable performance to the uncompressed baseline across various scenarios, achieving a 2x acceleration in inference time and an 8x reduction of memory costs for KV cache. © 2025 13th International Conference on Learning Representations, ICLR 2025. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-105010225903"
"Faysse, M.; Sibille, H.; Wu, T.; Omrani, B.; Viaud, G.; Hudelot, C.; Colombo, P.","Faysse, Manuel (58680476600); Sibille, Hugues (58191493000); Wu, Tony (59247370800); Omrani, Bilel (59247525100); Viaud, Gautier (56309978900); Hudelot, Céline (57203186077); Colombo, Pierre Jean A. (57216691461)","58680476600; 58191493000; 59247370800; 59247525100; 56309978900; 57203186077; 57216691461","ColPali: EFFICIENT DOCUMENT RETRIEVAL WITH VISION LANGUAGE MODELS","2025","","","","","64477","64502","9","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105010218160&partnerID=40&md5=3e12d8af62b59688e3f2b75737fdabb8","Documents are visually rich structures that convey information through text, but also figures, page layouts, tables, or even fonts. Since modern retrieval systems mainly rely on the textual information they extract from document pages to index documents -often through lengthy and brittle processes-, they struggle to exploit key visual cues efficiently. This limits their capabilities in many practical document retrieval applications such as Retrieval Augmented Generation (RAG). To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieval tasks spanning multiple domains, languages, and practical settings. The inherent complexity and performance shortcomings of modern systems motivate a new concept; doing document retrieval by directly embedding the images of the document pages. We release ColPali, a Vision Language Model trained to produce high-quality multi-vector embeddings from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically simpler, faster and end-to-end trainable. We release models, data, code and benchmarks under open licenses at https://hf.co/vidore. © 2025 13th International Conference on Learning Representations, ICLR 2025. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-105010218160"
"Tu, Y.; Zhang, C.; Guo, Y.; Chen, H.; Tang, J.; Zhu, H.; Zhang, Q.","Tu, Yi (58411864900); Zhang, Chong (57222873529); Guo, Ya (58414920800); Chen, Huan (58604460400); Tang, Jinyang (58414158500); Zhu, Huijia (57339992600); Zhang, Qi (57203621188)","58411864900; 57222873529; 58414920800; 58604460400; 58414158500; 57339992600; 57203621188","UNER: A Unified Prediction Head for Named Entity Recognition in Visually-rich Documents","2024","","","","","4890","4898","1","10.1145/3664647.3681473","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85209805958&doi=10.1145%2F3664647.3681473&partnerID=40&md5=53d59409fbb202b67328638c8c6b94ab","The recognition of named entities in visually-rich documents (VrD-NER) plays a critical role in various real-world scenarios and applications. However, the research in VrD-NER faces three major challenges: complex document layouts, incorrect reading orders, and unsuitable task formulations. To address these challenges, we propose a query-aware entity extraction head, namely UNER, to collaborate with existing multi-modal document transformers to develop more robust VrD-NER models. The UNER head considers the VrD-NER task as a combination of sequence labeling and reading order prediction, effectively addressing the issues of discontinuous entities in documents. Experimental evaluations on diverse datasets demonstrate the effectiveness of UNER in improving entity extraction performance. Moreover, the UNER head enables a supervised pre-training stage on various VrD-NER datasets to enhance the document transformer backbones and exhibits substantial knowledge transfer from the pre-training stage to the fine-tuning stage. By incorporating universal layout understanding, a pre-trained UNER-based model demonstrates significant advantages in few-shot and cross-linguistic scenarios and exhibits zero-shot entity extraction abilities. © 2024 Owner/Author.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85209805958"
"Jin, C.; Zhang, M.; Ma, W.; Li, Y.; Wang, Y.; Jia, Y.; Du, Y.; Sun, T.; Wang, H.; Fan, C.; Gu, J.; Chi, C.; Lv, X.; Li, F.; Xue, W.; Huang, Y.","Jin, Congyun (58745118500); Zhang, Ming (56768276700); Ma, Weixiao (58930106100); Li, Yujiao (58745950000); Wang, Yingbo (58109644500); Jia, Yabo (58931074600); Du, Yuliang (58745533100); Sun, Tao (58744907100); Wang, Haowen (58287265000); Fan, Cong (58745533200); Gu, Jinjie (57211945314); Chi, Chenfei (57191737330); Lv, Xiangguo (55945196200); Li, Fangzhou (57190192173); Xue, Wei (57217742952); Huang, Yiran (22979413800)","58745118500; 56768276700; 58930106100; 58745950000; 58109644500; 58931074600; 58745533100; 58744907100; 58287265000; 58745533200; 57211945314; 57191737330; 55945196200; 57190192173; 57217742952; 22979413800","RJUA-MedDQA: A Multimodal Benchmark for Medical Document Question Answering and Clinical Reasoning","2024","Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","","","","5218","5229","5","10.1145/3637528.3671644","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85203698869&doi=10.1145%2F3637528.3671644&partnerID=40&md5=9064ab6c1ac7af76210687f0a261c3aa","Recent advancements in Large Language Models (LLMs) and Large Multi-modal Models (LMMs) have shown potential in various medical applications, such as Intelligent Medical Diagnosis. Although impressive results have been achieved, we find that existing benchmarks do not reflect the complexity of real medical reports and specialized in-depth reasoning capabilities. In this work, we establish a comprehensive benchmark in the field of medical specialization and introduced RJUA-MedDQA, which contains 2000 real-world Chinese medical report images poses several challenges: comprehensively interpreting imgage content across a wide variety of challenging layouts, possessing the numerical reasoning ability to identify abnormal indicators and demonstrating robust clinical reasoning ability to provide the statement of disease diagnosis, status and advice based on a collection of medical contexts. We carefully design the data generation pipeline and proposed the Efficient Structural Restoration Annotation (ESRA) Method, aimed at restoring textual and tabular content in medical report images. This method substantially enhances annotation efficiency, doubling the productivity of each annotator, and yields a 26.8% improvement in accuracy. We conduct extensive evaluations, including few-shot assessments of 5 LMMs which are capable of solving Chinese medical QA tasks. To further investigate the limitations and potential of current LMMs, we conduct comparative experiments on a set of strong LLMs by using image-text generated by ESRA method. We report the performance of baselines and offer several observations: (1) The overall performance of existing LMMs is still limited; however LMMs more robust to low-quality and diverse-structured images compared to LLMs. (3) Reasoning across context and image content present significant challenges. We hope this benchmark helps the community make progress on these challenging tasks in multi-modal medical document understanding and facilitate its application in healthcare. Our dataset will be publicly available for noncommercial use at https://github.com/Alipay-Med/medDQA_benchmark.git © 2024 Copyright held by the owner/author(s).","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85203698869"
"Qi, S.; Wang, F.; Sun, H.; Ge, Y.; Xiao, B.","Qi, Siyang (59327037100); Wang, Fei (57211809022); Sun, Hongzhi (59563379000); Ge, Yang (59563379100); Xiao, Bo (35729594700)","59327037100; 57211809022; 59563379000; 59563379100; 35729594700","GVDIE: A Zero-Shot Generative Information Extraction Method for Visual Documents Based on Large Language Models","2024","","","","","","","0","10.1109/APSIPAASC63619.2025.10848851","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85218178853&doi=10.1109%2FAPSIPAASC63619.2025.10848851&partnerID=40&md5=50e22298e843e2b3f6b161c7655962c4","Document Information Extraction aims to extract entities and relationships from visually rich documents. Traditional methods require significant annotation and lack generality. In this paper, we propose GVDIE, a Generative Visual Document Information Extraction method that leverages large language models for zero-shot information extraction. This method aligns document-level multimodal features with text layout features, enhancing performance. We introduce a layout augment module for key text block features and transform the document information extraction from sequence labeling task to a generative one. This approach improves flexibility, efficiency, and adaptability in processing diverse, unstructured documents. © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85218178853"
"Li, X.; Wu, Y.; Jiang, X.; Guo, Z.; Gong, M.; Cao, H.; Liu, Y.; Jiang, D.; Sun, X.","Li, Xin (57775707200); Wu, Yunfei (57703120600); Jiang, Xinghua (57345097600); Guo, Zhihao (58950625500); Gong, Mingming (58154164300); Cao, Haoyu (57702859400); Liu, Yinsong (57223712406); Jiang, Deqiang (57219505693); Sun, Xing (56706675700)","57775707200; 57703120600; 57345097600; 58950625500; 58154164300; 57702859400; 57223712406; 57219505693; 56706675700","Enhancing Visual Document Understanding with Contrastive Learning in Large Visual-Language Models","2024","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","15546","15555","19","10.1109/CVPR52733.2024.01472","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207252335&doi=10.1109%2FCVPR52733.2024.01472&partnerID=40&md5=6090d8fde72b9f19f774464ced2f9fee","Recently, the advent of Large Visual-Language Models (LVLMs) has received increasing attention across various domains, particularly in the field of visual document understanding (VDU). Different from conventional vision-language tasks, VDU is specifically concerned with text-rich scenarios containing abundant document elements. Nevertheless, the importance of fine-grained features remains largely unexplored within the community of LVLMs, leading to suboptimal performance in text-rich scenarios. In this paper, we abbreviate it as the fine-grained feature collapse issue. With the aim of filling this gap, we propose a contrastive learning framework, termed Document Object COntrastive learning (DoCo), specifically tailored for the downstream tasks of VDU. DoCo leverages an auxiliary multimodal encoder to obtain the features of document objects and align them to the visual features generated by the vision encoder of LVLM, which enhances visual representation in text-rich scenarios. It can represent that the contrastive learning between the visual holistic representations and the multimodal fine-grained features of document objects can assist the vision encoder in acquiring more effective visual cues, thereby enhancing the comprehension of text-rich documents in LVLMs. We also demonstrate that the proposed DoCo serves as a plug-and-play pre-training method, which can be employed in the pre-training of various LVLMs without inducing any increase in computational complexity during the inference process. Extensive experimental results on multiple benchmarks of VDU reveal that LVLMs equipped with our proposed DoCo can achieve superior performance and mitigate the gap between VDU and generic vision-language tasks. © 2024 IEEE.","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85207252335"
"Liu, C.; Yin, K.; Cao, H.; Jiang, X.; Li, X.; Liu, Y.; Jiang, D.; Sun, X.; Xu, L.","Liu, Chaohu (58494040800); Yin, Kun (58621462700); Cao, Haoyu (57702859400); Jiang, Xinghua (57345097600); Li, Xin (57775707200); Liu, Yinsong (57223712406); Jiang, Deqiang (57219505693); Sun, Xing (56706675700); Xu, Linli (23669631700)","58494040800; 58621462700; 57702859400; 57345097600; 57775707200; 57223712406; 57219505693; 56706675700; 23669631700","HRVDA: High-Resolution Visual Document Assistant","2024","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","","","","15534","15545","14","10.1109/CVPR52733.2024.01471","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85207039033&doi=10.1109%2FCVPR52733.2024.01471&partnerID=40&md5=40d31de8b80f27e0a230a484c3e6753b","Leveraging vast training data, multimodal large language models (MLLMs) have demonstrated formidable general visual comprehension capabilities and achieved remarkable performance across various tasks. However, their performance in visual document understanding still leaves much room for improvement. This discrepancy is primarily attributed to the fact that visual document understanding is a fine-grained prediction task. In natural scenes, MLLMs typically use low-resolution images, leading to a substantial loss of visual information. Furthermore, general-purpose MLLMs do not excel in handling document-oriented instructions. In this paper, we propose a High-Resolution Visual Document Assistant (HRVDA), which bridges the gap between MLLMs and visual document understanding. This model employs a content filtering mechanism and an instruction filtering module to separately filter out the content-agnostic visual tokens and instruction-agnostic visual tokens, thereby achieving efficient model training and inference for high-resolution images. In addition, we construct a document-oriented visual instruction tuning dataset and apply a multi-stage training strategy to enhance the model's document modeling capabilities. Extensive experiments demonstrate that our model achieves state-of-the-art performance across multiple document understanding datasets, while maintaining training efficiency and inference speed comparable to low-resolution models. © 2024 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85207039033"
"van Landeghem, J.; Maity, S.; Banerjee, A.; Blaschko, M.; Moens, M.-F.; Llados, J.; Biswas, S.","van Landeghem, Jordy (57606479600); Maity, Subhajit (57209077786); Banerjee, Ayan (57215614021); Blaschko, Matthew B. (24829297300); Moens, Marie Francine (7005471657); Llados, Josep (6603062543); Biswas, Sanket (57226196113)","57606479600; 57209077786; 57215614021; 24829297300; 7005471657; 6603062543; 57226196113","DistilDoc: Knowledge Distillation for Visually-Rich Document Applications","2024","Lecture Notes in Computer Science","14807 LNCS","","","195","217","5","10.1007/978-3-031-70546-5_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204543479&doi=10.1007%2F978-3-031-70546-5_12&partnerID=40&md5=8bc596a8385fe97c8cf95e6893bc7c08","This work explores knowledge distillation (KD) for visually-rich document (VRD) applications such as document layout analysis (DLA) and document image classification (DIC). While VRD research is dependent on increasingly sophisticated and cumbersome models, the field has neglected to study efficiency via model compression. Here, we design a KD experimentation methodology† for more lean, performant models on document understanding (DU) tasks that are integral within larger task pipelines. We carefully selected KD strategies (response-based, feature-based) for distilling knowledge to and from backbones with different architectures (ResNet, ViT, DiT) and capacities (base-small-tiny). We study what affects the teacher-student knowledge gap and find that some methods (tuned vanilla KD, MSE, SimKD with an apt projector) can consistently outperform supervised student training. Furthermore, we design downstream task setups to evaluate covariate shift and the robustness of distilled DLA models on zero-shot layout-aware document visual question answering (DocVQA). DLA-KD experiments result in a large mAP knowledge gap, which unpredictably translates to downstream robustness, accentuating the need to further explore how to efficiently obtain more semantic document layout awareness.(† Code available at: https://github.com/Jordy-VL/DistilDoc_ICDAR24) © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.","Conference paper","Final","All Open Access; Green Accepted Open Access; Green Open Access","Scopus","2-s2.0-85204543479"
"Zhu, W.; Healey, J.; Zhang, R.; Wang, W.Y.; Sun, T.","Zhu, Wanrong (57219762251); Healey, Jennifer A. (7202424024); Zhang, Ruiyi (57204807127); Wang, William Yang (57233559700); Sun, Tong (57219699023)","57219762251; 7202424024; 57204807127; 57233559700; 57219699023","Automatic Layout Planning for Visually-Rich Documents with Instruction-Following Models","2024","","","","","167","172","1","10.18653/v1/2024.alvr-1.14","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85204479613&doi=10.18653%2Fv1%2F2024.alvr-1.14&partnerID=40&md5=95d14461e793380be8431d417ceee101","Recent advancements in instruction-following models have made user interactions with models more user-friendly and efficient, broadening their applicability. In graphic design, nonprofessional users often struggle to create visually appealing layouts due to limited skills and resources. In this work, we introduce a novel multimodal instruction-following framework for layout planning, allowing users to easily arrange visual elements into tailored layouts by specifying canvas size and design purpose, such as for book covers, posters, brochures, or menus. We developed three layout reasoning tasks to train the model in understanding and executing layout instructions. Experiments on two benchmarks show that our method not only simplifies the design process for non-professionals but also surpasses the performance of few-shot GPT-4V models, with mIoU higher by 12% on Crello (Yamaguchi, 2021). This progress highlights the potential of multimodal instruction-following models to automate and simplify the design process, providing an approachable solution for a wide range of design tasks on visually-rich documents. © 2024 Association for Computational Linguistics.","Conference paper","Final","","Scopus","2-s2.0-85204479613"
"Zhang, Y.; Chen, Y.; Zhu, J.; Xu, J.; Yang, S.; Wu, Z.; Huang, L.; Huang, Y.; Chen, S.","Zhang, Yunqi (59824887200); Chen, Yubo (58703966700); Zhu, Jingzhe (59171605100); Xu, Jinyu (57226332279); Yang, Shuai (57223114364); Wu, Zhaoliang (59172215300); Huang, Liang (58788673300); Huang, Yongfeng (14627673100); Chen, Shuai (59290566100)","59824887200; 58703966700; 59171605100; 57226332279; 57223114364; 59172215300; 58788673300; 14627673100; 59290566100","KnowVrDU: A Unified Knowledge-aware Prompt-Tuning Framework for Visually-rich Document Understanding","2024","","","","","9878","9889","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195973671&partnerID=40&md5=3c46de2049a3e2f0e7d57488555819d1","In Visually-rich Document Understanding (VrDU), recent advances of incorporating layout and image features into the pre-training language models have achieved significant progress. Existing methods usually developed complicated dedicated architectures based on pre-trained models and fine-tuned them with costly high-quality data to eliminate the inconsistency of knowledge distribution between the pre-training task and specialized downstream tasks. However, due to their huge data demands, these methods are not suitable for few-shot settings, which are essential for quick applications with limited resources but few previous works are presented. To solve these problems, we propose a unified Knowledge-aware prompt-tuning framework for Visual-rich Document Understanding (KnowVrDU) to enable broad utilization for diverse concrete applications and reduce data requirements. To model heterogeneous VrDU structures without designing task-specific architectures, we propose to reformulate various VrDU tasks into a single question-answering format with task-specific prompts and train the pre-trained model with the parameter-efficient prompt tuning method. To bridge the knowledge gap between the pre-training task and specialized VrDU tasks without additional annotations, we propose a prompt knowledge integration mechanism to leverage external open-source knowledge bases. We conduct experiments on several benchmark datasets in few-shot settings and the results validate the effectiveness of our method. © 2024 ELRA Language Resource Association: CC BY-NC 4.0.","Conference paper","Final","","Scopus","2-s2.0-85195973671"
"Ma, Y.; Zang, Y.; Chen, L.; Chen, M.; Jiao, Y.; Li, X.; Lu, X.; Liu, Z.; Ma, Y.; Dong, X.; Zhang, P.; Pan, L.; Jiang, Y.-G.; Wang, J.; Cao, Y.; Sun, A.","Ma, Yubo (57485133600); Zang, Yuhang (57219631614); Chen, Liangyu (58304845400); Chen, Meiqi (57271422200); Jiao, Yizhu (57211937974); Li, Xinze (58121980700); Lu, Xinyuan (58151160500); Liu, Ziyu (59505601200); Ma, Yan (59255153700); Dong, Xiaoyi (57215770324); Zhang, Pan (58262907700); Pan, Liangming (57114093200); Jiang, Yugang (14054081900); Wang, Jiaqi (59959599900); Cao, Yixin (57015851100); Sun, Aixin (7202552214)","57485133600; 57219631614; 58304845400; 57271422200; 57211937974; 58121980700; 58151160500; 59505601200; 59255153700; 57215770324; 58262907700; 57114093200; 14054081900; 59959599900; 57015851100; 7202552214","MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations","2024","Advances in Neural Information Processing Systems","37","","","","","8","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105000534551&partnerID=40&md5=692fa94137a0569250f096ab584e1b45","Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLONGBENCH-DOC, a long-context, multimodal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e., page number). Moreover, 33.7% of the questions are cross-page questions requiring evidence across multiple pages. 20.6% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9%, while the second-best, GPT-4V, scores 30.5%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs. © 2024 Neural information processing systems foundation. All rights reserved.","Conference paper","Final","","Scopus","2-s2.0-105000534551"
"Wang, Z.; Zhou, Y.; Wei, W.; Lee, C.-Y.; Tata, S.","Wang, Zilong (57221155226); Zhou, Yichao (57830742700); Wei, Wei (57203031737); Lee, Chenyu (57219741997); Tata, Sandeep (57871521300)","57221155226; 57830742700; 57203031737; 57219741997; 57871521300","VRDU: A Benchmark for Visually-rich Document Understanding","2023","Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","","","","5184","5193","16","10.1145/3580305.3599929","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171385003&doi=10.1145%2F3580305.3599929&partnerID=40&md5=6e50ecd48673fbe457973e788966222a","Understanding visually-rich business documents to extract structured data and automate business workflows has been receiving attention both in academia and industry. Although recent multi-modal language models have achieved impressive results, we find that existing benchmarks do not reflect the complexity of real documents seen in industry. In this work, we identify the desiderata for a more comprehensive benchmark and propose one we call Visually Rich Document Understanding (VRDU). VRDU contains two datasets that represent several challenges: rich schema including diverse data types as well as hierarchical entities, complex templates including tables and multi-column layouts, and diversity of different layouts (templates) within a single document type. We design few-shot and conventional experiment settings along with a carefully designed matching algorithm to evaluate extraction results. We report the performance of strong baselines and offer three observations: (1) generalizing to new document templates is still very challenging, (2) few-shot performance has a lot of headroom, and (3) models struggle with hierarchical fields such as line-items in an invoice. We plan to open source the benchmark and the evaluation toolkit. We hope this helps the community make progress on these challenging tasks in extracting structured data from visually rich documents. © 2023 Owner/Author.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85171385003"
"Lin, Z.; Wang, J.; Jin, L.","Lin, Zening (58614985200); Wang, Jiapeng (57219764141); Jin, Lianwen (7403329268)","58614985200; 57219764141; 7403329268","Visual information extraction deep learning method：a critical review; 视觉信息抽取的深度学习方法综述","2023","Journal of Image and Graphics","28","8","","2276","2297","4","10.11834/jig.220904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172105355&doi=10.11834%2Fjig.220904&partnerID=40&md5=eee4bfe7bddd9d269ba6c78ab466fbb1","A huge amount of big data-driven documents are required to be digitalized，stored and distributed in relation to images contexts. Such of application scenarios are concerned of document images-oriented key information，such as receipt understanding，card recognition，automatic paper scoring and document matching. Such process is called visual information extraction（VIE），which is focused on information mining，analysis，and extraction from visually rich documents. Documents-related text objects are diverse and varied，multi-language documents can be also commonly-used incorporated with single language scenario. Furthermore，text corpus differs from field to field. For example，a difference in the text content is required to be handled between legal files and medical documents. A complex layout may exist when a variety of visual elements are involved in a document，such as pictures，tables，and statistical curves. Unreadable document images are often derived and distorted from such noises like ink，wrinkles，distortion，and illumination. The completed pipeline of visual information extraction can be segmented into four steps：first，a pre-processing algorithm should be applied to remove the problem of interference and noise in a manner of correction and denoising. Second，document image-derived text strings and their locations contexts may be extracted in terms of text detection and recognition methods. Subsequently，multimodal feature extraction is required to perform high-level calculation and fusion of text，layout and visual features contained in visually rich documents. Finally，entity category parsing is applied to determine the category of each entity. Existed methods are mainly focused on the latter of two steps，while some take text detection and recognition into account. Early works are concerned of querying key information manually via rule-based methods. The effectiveness of these algorithms is quite lower，and they have poor generalization performance as well. The emerging deep learning technique-based feature extractors like convolutional neural networks and Transformers are linked with depth features for the optimization of performance and efficiency. In recent years，deep learning based methods have been widely applied in real scenarios. To sum up，we review deep-learning-based VIE methods and public datasets proposed in recent years，and these algorithms can be classified by their main characteristics. Recent deep-learning-based VIE methods proposed can be roughly categorized into six types of methods relevant to such contexts of grid-based，graph-neural-network-based （GNN-based），Transformer-based，end-to-end，few-shot，and the related others. Grid-based methods are focused on taking the document image as a two-dimensional matrix，pixels-inner text bounding box are filled with text embedding，and the grid representation can be formed for deep processing. Grid-based methods are often simple and have less computational cost. However，its representation ability is not strong enough，and features of text regions in small size may not be fully exploited. GNN-based methods take text segments as graph nodes，relations between segment coordinates are encoded for edge representations. Such graph convolution-related operations are applied for feature extraction further. GNN-based schemes achieve a good balance between cost and performance，but some characteristics of GNN itself like over-smoothing and gradient vanishing are often challenged to train the model. Transformer-based methods achieve outstanding performance through pretraining with a vast amount of data. These methods are preferred to have powerful generalizability，and it can be applied for multiple scenarios extended to other related document understanding tasks. However，these computational models are often costly and computing resources are required to be optimized. A more efficient architecture and pre-training strategy is still as a challenging problem to be resolved. The VIE is a mutual-benefited process，and text detection and recognition optical character recognition（OCR）are needed as prerequisites. The OCR-attainable problems like coordinate mismatches and text recognition errors will affect the following steps as well. Such end-to-end paradigms can be traced to optimize the OCR error accumulation to some extent. Few-shot methods-related structures can be used to enhance the generalization ability of models efficiently，and intrinsic features can be exploited to some extend in term of a small number of samples only. First，the growth of this research domain is reviewed and its challenging contexts can be predicted as well. Then，recent deep learning based visual information extraction methods and their contexts are summarized and analyzed. Furthermore，multiple categories-relevant methods are predictable，while the algorithm flow and technical development route of the representative models are further discussed and analyzed. Additionally，features of some public datasets are illustrated in comparison with the performance of representative models on these benchmarks. Finally，research highlights and limitations of each sort of model are laid out，and future research direction is forecasted as well. © 2023 Chinese Institute of Food Science and Technology. All rights reserved.","Article","Final","All Open Access; Bronze Open Access","Scopus","2-s2.0-85172105355"
"Yang, X.; Men, G.; Yin, C.","Yang, Xieliu (56523550400); Men, Guowen (58905620700); Yin, Chenyu (57219241993)","56523550400; 58905620700; 57219241993","A Deep Learning-based Multi-language Document Layout Analysis Method","2023","IET Conference Proceedings","2023","30","","193","197","0","10.1049/icp.2023.3309","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188269980&doi=10.1049%2Ficp.2023.3309&partnerID=40&md5=5be322e0204cf0536e5cc9ff22364849","Layout analysis of complex layout documents is significant for document understanding systems and is still a challenging task. A deep learning-based layout analysis method is proposed for contemporary printed documents in this paper. The network is developed from Mask-RCNN and the standard convolution structure in Mask-RCNN is replaced by deformable convolution network (DCN) to improve the location accuracy of the segmentation contour. Transfer learning is employed to solve the few-shot problem and the fine-tune mode is investigated. The network is pre-trained on the PubLayNet dataset and fine-tuned on the ICDAR2009 dataset. Experiments show that the proposed method outperforms the tested traditional method and is insensitive to language. © The Institution of Engineering & Technology 2023.","Conference paper","Final","","Scopus","2-s2.0-85188269980"
"","","","Findings of the Association for Computational Linguistics: EMNLP 2023","2023","","","","","","","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183298223&partnerID=40&md5=9ad2dccaec0b14e8534a3df71ae61891","The proceedings contain 1060 papers. The topics discussed include: what makes chain-of-thought prompting effective? a counterfactual study; MenatQA: a new dataset for testing the temporal comprehension and reasoning abilities of large language models; perceptual structure in the absence of grounding for LLMs: the impact of abstractedness and subjectivity in color language; a dataset for investigating the impact of context for offensive language detection in tweets; remember what you did so you know what to do next; an empirical study of multimodal model merging; learning to abstract with nonparametric variational information bottleneck; global structure knowledge-guided relation extraction method for visually-rich document; learning to compose representations of different encoder layers towards improving compositional generalization; and SELECTNOISE: unsupervised noise injection to enable zero-shot machine translation for extremely low-resource languages.","Conference review","Final","","Scopus","2-s2.0-85183298223"
"Fujinuma, Y.; Varia, S.; Sankaran, N.; Min, B.; Appalaraju, S.; Vyas, Y.","Fujinuma, Yoshinari (57209002950); Varia, Siddharth (57215135516); Sankaran, Nishant (57188726336); Min, Bonan (23480129200); Appalaraju, Srikar (57208012672); Vyas, Yogarshi (56285717600)","57209002950; 57215135516; 57188726336; 23480129200; 57208012672; 56285717600","A Multi-Modal Multilingual Benchmark for Document Image Classification","2023","","","","","14361","14376","4","10.18653/v1/2023.findings-emnlp.958","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183294531&doi=10.18653%2Fv1%2F2023.findings-emnlp.958&partnerID=40&md5=bf983c396f0c0302ed8303ef20473fbd","Document image classification is different from plain-text document classification and consists of classifying a document by understanding the content and structure of documents such as forms, emails, and other such documents. We show that the only existing dataset for this task (Lewis et al., 2006) has several limitations and we introduce two newly curated multilingual datasets (WIKI-DOC and MULTIEURLEXDOC) that overcome these limitations. We further undertake a comprehensive study of popular visually-rich document understanding or Document AI models in previously untested setting in document image classification such as 1) multi-label classification, and 2) zero-shot cross-lingual transfer setup. Experimental results show limitations of multilingual Document AI models on cross-lingual transfer across typologically distant languages. Our datasets and findings open the door for future research into improving Document AI models. © 2023 Association for Computational Linguistics.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85183294531"
"van Landeghem, J.; Tito, R.; Borchmann, Ł.; Pietruszka, M.; Jurkiewicz, D.; Powalski, R.; Józiak, P.; Biswas, S.; Coustaty, M.; Stanisławek, T.","van Landeghem, Jordy (57606479600); Tito, Rubèn (57215081168); Borchmann, Łukasz (57192829414); Pietruszka, Michał (57219766139); Jurkiewicz, Dawid (57219588709); Powalski, Rafał (57219636188); Józiak, Paweł (56531999800); Biswas, Sanket (57226196113); Coustaty, Mickaël (25631940800); Stanisławek, Tomasz (57188974746)","57606479600; 57215081168; 57192829414; 57219766139; 57219588709; 57219636188; 56531999800; 57226196113; 25631940800; 57188974746","ICDAR 2023 Competition on Document UnderstanDing of Everything (DUDE)","2023","Lecture Notes in Computer Science","14188 LNCS","","","420","434","9","10.1007/978-3-031-41679-8_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173577054&doi=10.1007%2F978-3-031-41679-8_24&partnerID=40&md5=fbc65088d0bc1f3f648973806a72f613","This paper presents the results of the ICDAR 2023 competition on Document UnderstanDing of Everything. DUDE introduces a new dataset comprising 5 K visually-rich documents (VRDs) with 40 K questions with novelties related to types of questions, answers, and document layouts based on multi-industry, multi-domain, and multi-page VRDs of various origins and dates. The competition was structured as a single task with a multi-phased evaluation protocol that assesses the few-shot capabilities of models by testing generalization to previously unseen questions and domains, a condition essential to business use cases prevailing in the field. A new and independent diagnostic test set is additionally constructed for fine-grained performance analysis. A thorough analysis of results from different participant methods is presented. Under the newly studied settings, current state-of-the-art models show a significant performance gap, even when improving visual evidence and handling multi-page documents. We conclude that the DUDE dataset proposed in this competition will be an essential, long-standing benchmark to further explore for achieving improved generalization and adaptation under low-resource fine-tuning, as desired in the real world. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.","Conference paper","Final","","Scopus","2-s2.0-85173577054"
"Wang, Z.; Zhang, Z.; Devlin, J.; Lee, C.-Y.; Su, G.; Zhang, H.; Dy, J.; Perot, V.; Pfister, T.","Wang, Zifeng (57215324239); Zhang, Zizhao (57020905500); Devlin, Jacob (54879967400); Lee, Chenyu (57219741997); Su, Guolong (57383417500); Zhang, Hao (57192482935); Dy, Jennifer G. (6603643756); Perot, Vincent (57210417906); Pfister, Tomas (36598926300)","57215324239; 57020905500; 54879967400; 57219741997; 57383417500; 57192482935; 6603643756; 57210417906; 36598926300","QueryForm: A Simple Zero-shot Form Entity Query Framework","2023","Proceedings of the Annual Meeting of the Association for Computational Linguistics","","","","4146","4159","3","10.18653/v1/2023.findings-acl.255","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173232752&doi=10.18653%2Fv1%2F2023.findings-acl.255&partnerID=40&md5=73dc0155db9d853b9107d20b0cf1988b","Zero-shot transfer learning for document understanding is a crucial yet under-investigated scenario to help reduce the high cost involved in annotating document entities. We present a novel query-based framework, QueryForm, that extracts entity values from form-like documents in a zero-shot fashion. QueryForm contains a dual prompting mechanism that composes both the document schema and a specific entity type into a query, which is used to prompt a Transformer model to perform a single entity extraction task. Furthermore, we propose to leverage large-scale query-entity pairs generated from form-like webpages with weak HTML annotations to pre-train QueryForm. By unifying pre-training and fine-tuning into the same query-based framework, QueryForm enables models to learn from structured documents containing various entities and layouts, leading to better generalization to target document types without the need for target-specific training data. QueryForm sets new state-of-the-art average F1 score on both the XFUND (+4.6%∼10.1%) and the Payment (+3.2%∼9.5%) zero-shot benchmark, with a smaller model size and no additional image input. © 2023 Association for Computational Linguistics.","Conference paper","Final","All Open Access; Gold Open Access","Scopus","2-s2.0-85173232752"
"Kong, Y.; Luo, C.; Ma, W.; Zhu, Q.; Zhu, S.; Yuan, N.; Jin, L.","Kong, Yuxin (57702173100); Luo, Canjie (57205363846); Ma, Weihong (57219766488); Zhu, Qiyuan (57702440000); Zhu, Shenggao (57223737809); Yuan, Nicholas Jing (55818206900); Jin, Lianwen (7403329268)","57702173100; 57205363846; 57219766488; 57702440000; 57223737809; 55818206900; 7403329268","Look Closer to Supervise Better: One-Shot Font Generation via Component-Based Discriminator","2022","Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2022-June","","","13472","13481","71","10.1109/CVPR52688.2022.01312","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136216918&doi=10.1109%2FCVPR52688.2022.01312&partnerID=40&md5=6b35413234479684f93c4e5ff4a74a52","Automatic font generation remains a challenging research issue due to the large amounts of characters with complicated structures. Typically, only a few samples can serve as the style/content reference (termed few-shot learning), which further increases the difficulty to preserve local style patterns or detailed glyph structures. We investigate the drawbacks of previous studies and find that a coarsegrained discriminator is insufficient for supervising a font generator. To this end, we propose a novel Component-Aware Module (CAM), which supervises the generator to decouple content and style at a more fine-grained level, i.e., the component level. Different from previous studies struggling to increase the complexity of generators, we aim to perform more effective supervision for a relatively simple generator to achieve its full potential, which is a brand new perspective for font generation. The whole framework achieves remarkable results by coupling component-level supervision with adversarial learning, hence we call it Component-Guided GAN, shortly CG-GAN. Extensive experiments show that our approach outperforms state-of-the-art one-shot font generation methods. Furthermore, it can be applied to handwritten word synthesis and scene text image editing, suggesting the generalization of our approach. © 2022 IEEE.","Conference paper","Final","","Scopus","2-s2.0-85136216918"
"","","","SDU 2022 - Proceedings of the Workshop on Scientific Document Understanding, co-located with 36th AAAI Conference on Artificial Inteligence, AAAI 2022","2022","CEUR Workshop Proceedings","3164","","","","","0","","https://www.scopus.com/inward/record.uri?eid=2-s2.0-105003105894&partnerID=40&md5=4750b8b96626f36ee6782c65157e2ac1","The proceedings contain 30 papers. The topics discussed include: neural architectures for biological inter-sentence relation extraction; coherence-based second chance autoencoders for document understanding; longitudinal citation prediction using temporal graph neural networks; zero-shot and few-shot classification of biomedical articles in context of the COVID-19 pandemic; fine-grained intent classification in the legal domain; domain adaptive pretraining for multilingual acronym extraction; sequence labeling for citation field extraction from Cyrillic script references; acronym identification using transformers and flair framework; and applying multi-task reading comprehension in acronym disambiguation.","Conference review","Final","","Scopus","2-s2.0-105003105894"
"Wei, M.; He, Y.; Zhang, Q.","Wei, Mengxi (57218714204); He, Yifan (57211947521); Zhang, Qiong (57208922961)","57218714204; 57211947521; 57208922961","Robust Layout-aware IE for Visually Rich Documents with Pre-trained Language Models","2020","","","","","2367","2376","42","10.1145/3397271.3401442","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090156625&doi=10.1145%2F3397271.3401442&partnerID=40&md5=82d8bdc686aca602fc3a683fce252d92","Many business documents processed in modern NLP and IR pipelines are visually rich: in addition to text, their semantics can also be captured by visual traits such as layout, format, and fonts. We study the problem of information extraction from visually rich documents (VRDs) and present a model that combines the power of large pre-trained language models and graph neural networks to efficiently encode both textual and visual information in business documents. We further introduce new fine-tuning objectives to improve in-domain unsupervised fine-tuning to better utilize large amount of unlabeled in-domain data. We experiment on real world invoice and resume data sets and show that the proposed method outperforms strong text-based RoBERTa baselines by 6.3% absolute F1 on invoices and 4.7% absolute F1 on resumes. When evaluated in a few-shot setting, our method requires up to 30x less annotation data than the baseline to achieve the same level of performance at ∼90% F1. © 2020 ACM.","Conference paper","Final","","Scopus","2-s2.0-85090156625"
